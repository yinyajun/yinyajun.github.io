<?xml version="1.0" encoding="utf-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Yin记</title><link>https://yinyajun.github.io/</link><description>yajun的学习笔记</description><generator>Hugo 0.135.0 https://gohugo.io/</generator><language>zh-CN</language><managingEditor>skyblueice234@gmail.com (雅俊)</managingEditor><webMaster>skyblueice234@gmail.com (雅俊)</webMaster><lastBuildDate>Wed, 07 Jan 2026 10:39:38 +0000</lastBuildDate><atom:link rel="self" type="application/rss+xml" href="https://yinyajun.github.io/rss.xml"/><item><title>《深度关系》金句摘录-第六章</title><link>https://yinyajun.github.io/posts/emotion/deep-relation06/</link><guid isPermaLink="true">https://yinyajun.github.io/posts/emotion/deep-relation06/</guid><pubDate>Wed, 07 Jan 2026 07:20:10 +0000</pubDate><author>skyblueice234@gmail.com (雅俊)</author><description>&lt;p>人们经常说，自己是出于善意，为了照顾对方的感受才不给予批评的反馈。但是，这真的是为了对方，还是为了我们自己呢？&lt;/p>
&lt;p>杰西卡让自己陷入困境的一个原因，是她没有对自己日益增长的沮丧情绪给予足够的关注。
相反，她没有重视自己的情绪，这是许多人一直在做的事情。
然而，这句俗话说得很对——“掌控自己的情绪，否则情绪就会掌控你”&lt;/p>
&lt;p>在任何关系里，​“刺痛”都是不可避免的。你为对方做了一件事，但你觉得他没有完全看到你的付出，你会说些什么吗？如果说了，他们会认为你这么做很小气吗？
或者，也许你分享了一些私人的话题，但对方没有注意到，你感到有些失望。这些都不是严重的冲突。有些事情会过去，但还有些事情会让你耿耿于怀，如果不加以处理，就可能变成相当大的问题—我们称之为“剧痛”​。&lt;/p>
&lt;p>在一段关系最初建立的时候，双方都会把最好的一面表现出来。
但随着彼此了解的加深，一方难免会做出让另一方不高兴的事。
我们每个人都有自己理解问题、提出问题和解决问题的方式，
在组织环境中，我们还都有着各自的工作方式。这些差异可能是可以共存的，也可能不行。
这是个两难的问题：你想更充分地做真实的自己，但如果做自己会给别人带来问题，那该怎么办?&lt;/p>
&lt;p>人际关系问题是不可避免的，是建立和维持关系中的正常组成部分。但是，在问题演变成严重冲突之前提出来是更容易的。
如果“刺痛”发现得早，双方都不太可能陷入消极的情绪里。但如果不加处理，令人烦恼的问题就会恶化。这样当问题最终被提出来的时候，往往已经发展到比最初的事件更加严重，还可能让好几个问题纠缠在一起。
(这次吵架表面上是关于牛奶的，但问题的根源不是牛奶——牛奶代表了长时间积攒起来的不满。)&lt;/p>
&lt;p>为什么不愿意说出“刺痛”？&lt;/p>
&lt;ul>
&lt;li>人们常常不愿意说出“刺痛”​，是因为他们担心这样可能会显得自己过于敏感、心胸狭隘。你可能认识某些这样的人，哪怕是无伤大雅的评论也会让他们备受冒犯，你不想像他们一样。或者，你可能会想，这根本不重要。有时的确如此，但有时如果你深入探索一下，你就会发现，这件事对你来说，比你之前意识到的更重要。&lt;/li>
&lt;li>还有一个原因让许多人不愿意说出“刺痛”​，那就是他们担心说出来会让事情变得更糟糕。你的抱怨是否会招致对方的报复？是否会导致一系列其他问题？或者，你不愿开口，是因为你认为这段关系（或者对方）很脆弱?&lt;/li>
&lt;li>我们不愿在出现问题的早期提出“刺痛”​，还有最后一个原因，那就是我们假定对方并没有恶意。我们会想：如果他们不是故意让我不舒服，那我为什么要不舒服？
(&lt;strong>他人的意图与他们行为的结果之间是有区别的&lt;/strong>。埃琳娜的烦恼本身是真实的——她不需要证明自己感受的存在是合理的。)&lt;/li>
&lt;/ul>
&lt;p>许多“刺痛”会自行消失，但你可以问问自己，这次的“刺痛”会持续存在吗？这种“刺痛”与其他问题有关系吗？&lt;/p>
&lt;p>一旦“刺痛”这样发展下去，就有可能变成“剧痛”​。​“剧痛”比“刺痛”更严重，
因为在这种情况下，不但你的情绪更强烈，你也更有可能对对方产生消极的印象。&lt;/p>
&lt;p>当“刺痛”变成“剧痛”时，我们会对问题的来龙去脉产生一些内心叙事，其中可能会包括对&lt;strong>对方的消极假设&lt;/strong>。&lt;/p>
&lt;p>此外，一旦心里有了一个消极的叙事，人们就会产生选择性收集信息的倾向。
或者，用我们的话说，人们会“构建支持我们观点的事例”​。
实际上，无论你认为自己有多客观，每个人都很容易受到证实偏差的影响。
当你产生一种信念（甚至直觉）的时候，你就会产生一种倾向，
更留意支持这种信念的事例，不关注任何与之不符的例子。&lt;/p>
&lt;p>“笑声让两个人之间的距离最短。​”
分享一个笑话或者有趣的评论，可以拉近我们彼此的距离。幽默可以放松心态、振奋精神。
&lt;strong>当我们嬉戏和玩笑时，不仅能更好地了解彼此，也能体验到一种特殊的自由。&lt;/strong>&lt;/p>
&lt;p>用幽默传达信息有一个问题，因为幽默具有内在的模糊性。这并不是说幽默在任何情况下都没有作用。但是，你必须对当时的情况保持敏感。你的“刺痛”有多严重？​（请记住，你的反应可能包含你没有意识到的信息。​）对方的幽默感怎么样？
有些人喜欢诙谐的回应，即使这样的回应让他们有些难堪，而有些人可能会往心里去。你还必须考虑你们之间的关系如何。如果对方知道你是接纳他的，开个玩笑可能会起到很好的效果。最后，你还要考虑场合。如果你考虑了所有这些因素，幽默就能起到积极的作用。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>深化学习&lt;/strong>&lt;/p>
&lt;p>&lt;strong>自我反思&lt;/strong>&lt;/p>
&lt;p>&lt;strong>1. 想象自己是埃琳娜。&lt;/strong>
如果你在团队会议上遇到了她那样的情况，也就是你的观点常常被忽视，但之后又被他人重述，你会如何回应？你会听之任之，还是会说些什么？在高管委员会会议之后，当桑杰不理会你的担忧时，你会做什么？请详细地思考一下你会做什么、说什么。&lt;/p>
&lt;p>&lt;strong>2. 处理“刺痛”：&lt;/strong>
请回想一些你在过去感受过的“刺痛”。&lt;/p>
&lt;p>● 你倾向于如何回应？你倾向于忍受、忽视、找机会回敬对方，还是生气？&lt;/p>
&lt;p>&lt;strong>3. 重要的关系：&lt;/strong>
在重要的关系里，现在有没有让你“刺痛”的事？是什么原因导致你不把这个问题说出来？&lt;/p>
&lt;p>&lt;strong>4. 运用幽默：&lt;/strong>
你倾向于怎样运用幽默？当你运用幽默的时候会发生什么？你有没有一些使用幽默的不良方式？你有善于运用幽默的朋友吗？他们是怎么让幽默发挥作用的？&lt;/p>
&lt;p>&lt;strong>应用&lt;/strong>&lt;/p>
&lt;p>在上面的问题 3 中，如果你在一段重要的关系中发现了一个挥之不去的“刺痛”，请把这个问题说出来。&lt;/p>
&lt;p>在接下来的几周里，请留意你感觉“刺痛”的时刻。你能分清哪些“刺痛”不必放在心上，哪些“刺痛”应该提出来吗？从你做的选择中，你能看出什么模式吗？认识到这些模式之后，你打算采取什么行动？&lt;/p>
&lt;p>你是否认识这样的人，当你向他们表达“刺痛”的时候，他们的态度不是很好？例如，他们是不是会不予理解，指责你太敏感，或者变得咄咄逼人？如果这些反应让你不愿意分享“刺痛”，那你该怎样提出这个问题，从而改善你们的关系？&lt;/p>
&lt;p>如果你经常使用幽默，那就去找几个了解你的人，问问你说的话是否总能取得你想要的效果。你可以问一些详细的问题，问问什么时候有效，什么时候没有效果。&lt;/p>
&lt;p>你是否认识这样的人，他们使用幽默的方式让你不舒服？可能是他们的幽默里有一些贬低人的成分，或者他们可能用幽默来间接地传达信息。这不是一个大问题，但越来越严重的“刺痛”会让你有一些不确定的感觉，让你们无法像你想要的那样亲密。请想办法提出这个问题，以改善你们的关系，并按计划坚持到底。&lt;/p>
&lt;p>&lt;strong>理解&lt;/strong>&lt;/p>
&lt;p>通过这些讨论，你已经开始为建立更深刻的关系扫清障碍了。事情进展如何？关于自己以及建立人际关系，你学到了什么？&lt;/p>
&lt;p>&lt;strong>注意：&lt;/strong>
你在尝试运用所学的时候，可能最初效果不尽如人意。最重要的是你从这样的尝试中（以及从对方身上）能学到什么，包括你在解决问题的过程中培养出来的技能。&lt;/p>
&lt;/blockquote></description><category domain="https://yinyajun.github.io/categories/%E6%83%85%E6%84%9F/">情感</category><category domain="https://yinyajun.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%85%B3%E7%B3%BB/">深度关系</category></item><item><title>《深度关系》金句摘录-第五章</title><link>https://yinyajun.github.io/posts/emotion/deep-relation05/</link><guid isPermaLink="true">https://yinyajun.github.io/posts/emotion/deep-relation05/</guid><pubDate>Wed, 07 Jan 2026 06:20:10 +0000</pubDate><author>skyblueice234@gmail.com (雅俊)</author><description>&lt;p>关系要想持久，影响力就必须达到平衡或匹配。&lt;/p>
&lt;p>自我表露、支持、信任以及进一步的表露——这种循环是平衡的基础。随着关系双方对彼此了解的增进，他们会利用对彼此的了解来推进了解的过程。我们认为，无论人与人之间的了解有多深，总有更深层的东西有待发现。&lt;/p>
&lt;p>建立稳固而有意义的关系，其目的不在于一股脑地自我表露，或者为深化关系而深化关系，
而在于&lt;strong>考虑每个人的需求&lt;/strong>，这样一来，双方的需求就能以相对平衡的方式得到满足。&lt;/p>
&lt;p>&lt;strong>所有的关系里都有权衡取舍，但要让一段关系维持下去，每个人的需求都要得到足够的满足，而且每个人都必须舍弃一些东西。从长期来看，收获的益处必须超过付出的代价。&lt;/strong>
随着关系的发展，双方都会让对方更充分地了解自己，这样两个人就能学着如何提高收益、减少代价。&lt;/p>
&lt;p>&lt;strong>健康关系的核心，是两人之间大致平等，这就产生了一种“公平”的感觉。&lt;/strong>
即使你得到的益处比代价多，如果你认为对方得到的好处更多，你最终也会感觉被利用了。
你不必时时刻刻地做成本收益分析，也不必每时每刻都达到绝对的平衡。重要的是，随着时间的推移，两人都认为彼此的关系是大致平等的。&lt;/p>
&lt;p>&lt;strong>评估关系里的收益与代价，并不是一种理性的加减法，也不可能是。每个人所看重的东西都是非常主观的。&lt;/strong>
例如，亚当非常看重工作的挑战，认为不能放松地与妻儿共进晚餐只是极小的代价。
另一个人可能非常看重与家人吃晚餐的时间，愿意在5:30下班，牺牲自己职业发展的机会。&lt;/p>
&lt;p>社会价值观、成长背景和个人经历都会影响一个人如何看待关系中的代价与收益。&lt;/p>
&lt;p>健康关系的核心，是两人之间大致平等，这就产生了一种“公平”的感觉。&lt;/p>
&lt;p>最重要的是，关系中的双方都要清楚自己想要什么，也要知道对方想要什么。&lt;/p>
&lt;p>与往常一样，情绪是极好的指示牌。只要我们相信，我们的感受指明了我们真正想要的东西，我们就有了一个很好的开始&lt;/p>
&lt;p>&lt;strong>不幸的是，我们往往会从自身的需求和价值观出发，来评判别人提出的需求和抱怨是否“合理”&lt;/strong> ​。这正是麦蒂的母亲和亚当对于麦蒂的问题所做出的反应。这种做法会增加隔阂、减少理解。
麦蒂的需求本身是正当的，而且她想要得到倾听和理解。
可是，&lt;strong>关系中的双方不仅有责任确保自身的收益与代价保持平衡，也需要确保对方的平衡&lt;/strong>。不幸的是，亚当似乎不关心麦蒂的收益与代价的失衡。&lt;/p>
&lt;p>亚当怎么做才能让麦蒂的收益与代价恢复平衡呢？&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>第一步是讨论彼此的需求和不满。&lt;/strong>
他可以坦白自己害怕失去这种对于他很有利的安排，然后暂时把这个关注点放在一边，帮助麦蒂探索她的沮丧。
亚当的好奇心会让他更好地了解麦蒂，让麦蒂更有可能感到被充分地理解。
&lt;strong>表现出你对对方感受的理解，是满足对方需求的一种形式。&lt;/strong> ​（&lt;em>表达理解是“满足情感需求”的组成部分&lt;/em>，我们在第9章会详细解释这个话题。​）
但是，我们不认为亚当应为夫妻关系紧张负全部的责任。麦蒂也助长了这种紧张，我们会在稍后再讨论这一点。
&lt;strong>当我们能够讨论自己的需求和不满时，我们就更有可能找到适合每个人的解决方案。&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>第二步是重新找回平衡。&lt;/strong> 在澄清了彼此的需求之后，就要重新评估过去做出的安排。
曾经感觉合适的安排，不一定能一直合适。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在所有的关系里，各种条件都会发生变化——会出现新的工作机会，家庭成员会生病，人的年龄也会增长。
如果个人被过去的安排所束缚，他们自身和关系的成长就会面临停滞的风险。
在最好的情况下，双方会发现自己的新需求，寻求更多其他的益处，并且学着处理并放下之前的限制，关系就会不断地向前发展。
不过，如果双方成长的速度和方向不同，就会出现问题，给关系造成压力。
糟糕的是一方或双方为了回避冲突而停止成长。
当关系出现不平衡的时候，唯一有效的前进方式是直面这些变化，理解变化的影响，并一同探索如何解决问题。
重新评估关系中的安排并不容易。这可能会导致改变，而改变通常会引起抗拒：这样会导致什么结果？会不会让我必须放弃一些对我很重要的东西，或者导致一些我没准备好付出的代价？这样做也会带来不可预测性（你要如何回应呢）​，
也可能导致内疚或指责（我们以前为什么不这样做）​。请做好心理准备，因为这不是一次讨论就能解决的问题，而且在问题得到解决之前，你们会感到难过和沮丧。重新评估关系是至关重要的，不要认为这是一件容易的事。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>为什么亚当和麦蒂会被过去的安排束缚住？
难道只是因为亚当拒绝承认这种安排不再合适了吗？
是因为麦蒂害怕导致冲突而不愿意态度再强硬一些吗？
虽然所有这些因素都有影响，但还有一个&lt;strong>更大、更基础的问题：在面临争议时，他们都难以影响对方。&lt;/strong>
&lt;strong>他们面临的困境在一定程度上是由地位差异导致的。&lt;/strong> 如果一个人（通常是女性）辞去工作，成为全职父母，双方关系权力的平衡就会发生变化。&lt;/p>
&lt;p>大多数关系中都存在影响力的差异。不过，小的差异很少会妨碍坦诚交流、有效解决问题。然而，过大的影响力差异往往会导致不良的循环。
可惜的是，这种关系动力会成为自我实现的预言。如果影响力大的人认为，影响力小的人不能为关系做出什么贡献，那他为什么要听后者的话呢？
由于人们不愿意处在依赖他人的位置上（权力小的人所处的位置）​，他们就会倾向于退缩。当他们退缩的时候，他们为关系做的就更少了，从而强化了这种“没有贡献”的看法。&lt;/p>
&lt;p>亚当和麦蒂（在无意识中）在他们之间&lt;strong>共同制造了巨大的影响力差异&lt;/strong>。亚当没有认真对待麦蒂的担忧，只提出了敷衍了事的解决方案，让麦蒂感到被误解、不被重视、无力。不但如此，亚当还试图要求她遵守他们之前做出的安排，通过生硬的回应，以及不承认麦蒂所说的问题，亚当还表明了他不是很愿意受麦蒂的影响。但是，麦蒂并没有充分表达她的不满，并且选择去洗衣服，也就是做出了让步，这进一步加剧了他们之间的影响力差异。她任由亚当的逻辑论证占据上风，掩盖她情绪的价值，并且认可了亚当的看法——他们过去已经做好了安排。这样一来，她也失去了自己的影响力。&lt;/p>
&lt;p>&lt;strong>这种审视我们如何沟通以及解决共同问题的能力，是建立深刻关系所需的、最关键的能力之一。&lt;/strong>
这种能力可以帮助我们解决眼前的具体问题，并且让未来的问题解决变得更容易。&lt;/p>
&lt;p>在产生冲突的时候，如果你们不愿意把对关系的承诺放在重要的位置上，你们就很难取得好的结果，
这会使得你们更难以重视关系。突然间，你们就陷入负强化的循环里了。从另一个角度来说，表明对关系的承诺，可以开启一个重要的正强化循环。
我们的承诺和投入的越多，结果就可能越好，也就更容易增强对于关系的承诺。&lt;/p>
&lt;p>不过，影响力更大的人更容易带头做出改变。尽管让影响力小的人采取主动很难，但也是可能的。
麦蒂要做的第一步，就是不要再放弃自己拥有的权力。&lt;strong>人们经常放弃自己的影响力，往往还没有意识到自己在这样做&lt;/strong>。&lt;/p>
&lt;p>放弃影响力的十种方式
● 假定自己的需求不如别人的重要。
● 不听从自己的感受。
● 允许自己说话被打断。
● 在有人不同意你的看法时让步。
● 回避冲突——不提出反对意见，维持表面的和谐。
● 不给予反馈，假定问题可能在于自己。
● 关心自己是否能得到别人的喜爱、认可，并将这一点视为最重要的事情。
● 认为自己说的话无关紧要。
● 不承认自己的成就。
● 除非自己有解决方法，否则不愿指出问题。
对于权力小的人来说，这些信念或行为会让他们难以提出困难的问题，并坚持为解决问题而努力。
&lt;strong>然而，最严重的限制性因素是对冲突的恐惧——认为冲突是关系有问题的表现，或者认为一旦发生冲突，分歧就会升级，并且对关系造成永久性的损害，或者会毁掉关系。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;h4 id="深化学习">深化学习&lt;/h4>
&lt;h5 id="自我反思">自我反思&lt;/h5>
&lt;p>1.把自己放在亚当或麦蒂的位置上。如果你是亚当：你和麦蒂在婚姻初期达成了一个协议，你认为你们可以按照这种安排做事。这种安排对你很有利，而麦蒂想要改变现状。如果你是麦蒂：多年前你们达成了一个协议，但当时的情况不同，而这种安排现状对你来说不再合适了。
● 你会有什么感受？&lt;/p>
&lt;p>● 你认为你会怎样回应？你可能会怎么做？&lt;/p>
&lt;p>2.影响力的差异：请把亚当和麦蒂陷入的不良互动看作他们之间影响力差异的结果——麦蒂的影响力远比亚当小。如果你是麦蒂，你会怎样回应、怎么做？如果你是亚当呢？&lt;/p>
&lt;p>3.让双方满意：请从你在第2章里找出的重要关系里选出一段关系，并写下：&lt;/p>
&lt;p>● 在这段关系里，你的满足（收益）来自何处？&lt;/p>
&lt;p>● 这段关系有什么局限（代价）​？&lt;/p>
&lt;p>● 在你看来，对方认为这段关系对他自己有哪些益处？&lt;/p>
&lt;p>● 你认为对方会遇到哪些局限（代价）​？&lt;/p>
&lt;p>● 你觉得这段关系在多大程度上是平等、平衡的？你们在多大程度上以平衡的方式满足了彼此的需求？&lt;/p>
&lt;p>根据这项对你自己、对这段重要关系的评估，你们两人之间是否存在重大的影响力差异？如果有，你认为这种差异的根源是什么？&lt;/p>
&lt;p>4.相互影响：本章也强调了每个人影响对方的能力。对于上面讨论的关系：
● 你认为你对对方的影响力有多大？&lt;/p>
&lt;p>● 你有多愿意受到对方的影响？&lt;/p>
&lt;p>● 总体而言，你们之间谁的影响力更大？&lt;/p>
&lt;p>如果你在最后一个问题上选择了1或2，请思考：你的行为或他的行为是如何导致你的影响力更大的？&lt;/p>
&lt;p>如果你选择了4或5，请思考：你的行为或他的行为是如何导致他的影响力更大的？&lt;/p>
&lt;p>这种权力差异对你们的关系造成了什么影响（如果有影响的话）​？&lt;/p>
&lt;p>5.放弃影响力：我们在前文列出了十种放弃影响力的方式。那些描述符合你的情况吗？如果是这样的话，请思考为什么会这样。如果不那样做，你担心会发生什么？&lt;/p>
&lt;h5 id="应用">应用&lt;/h5>
&lt;p>在你选择的重要关系中，如果你发现你与对方之间存在满意度或影响力上的差异，请与对方讨论一下这个问题。他们是否也有同样的看法？讨论一下怎样才能减少这种差异。&lt;/p>
&lt;p>请注意，在这次讨论中，你会用上在前四章里学到的东西。你需要表达自己的需求、感受，以及你希望这次谈话会对你们的关系产生什么影响。&lt;/p>
&lt;p>请与对方分享你通常会以哪些方式放弃影响力。问问你生活中的一个重要的人，看看他们是否也有同样的看法。如果他们也这样认为，那他们可以怎样帮助你？&lt;/p>
&lt;h5 id="理解">理解&lt;/h5>
&lt;p>这样的讨论带来了哪些影响？你对自己有了哪些了解？这些了解怎样影响了你的关系？这种讨论让你们以后进行类似的讨论变得更容易了，还是更难了？&lt;/p>
&lt;p>你们不止是在讨论影响力，也是在讨论你们如何影响彼此。你们在多大程度上愿意受到对方的影响？根据你学到的东西，你会怎样改变自己的行为？&lt;/p>
&lt;/blockquote></description><category domain="https://yinyajun.github.io/categories/%E6%83%85%E6%84%9F/">情感</category><category domain="https://yinyajun.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%85%B3%E7%B3%BB/">深度关系</category></item><item><title>《深度关系》金句摘录-第四章</title><link>https://yinyajun.github.io/posts/emotion/deep-relation04/</link><guid isPermaLink="true">https://yinyajun.github.io/posts/emotion/deep-relation04/</guid><pubDate>Wed, 07 Jan 2026 05:20:10 +0000</pubDate><author>skyblueice234@gmail.com (雅俊)</author><description>&lt;p>要建立深度关系，不可能仅凭你一个人的努力。如果一个人从不提起闲聊之外的话题，你就无法与他建立更亲密的关系。&lt;/p>
&lt;p>共情不仅表达了你理解对方的感受，也表达了你认同对方的感受。用俗话说，就是你能与他们“感同身受”​。重要的是要注意，要与他人共情，你不必与他们有一模一样的经历&lt;/p>
&lt;p>同情是指承认某人的痛苦，并提供安慰和支持。这并不一定意味着认同他们的感受。同情也常常与怜悯联系在一起，这让很多人觉得自己更加渺小。同情与共情不一样，它并不鼓励对方更多地自我表露。事实上，同情有时会起到相反的作用，因为许多人不喜欢别人“为他们感到难过”​。&lt;/p>
&lt;p>毫无疑问，你肯定会有这样的互动经历：你想要了解更多，但对方却不愿多说。你表达关注和自我表露是很重要，但其作用也有限度。你还有其他的选择。在这种情况下，你首先要了解对方的情况，弄清如何与他们沟通。只有这样，你们才能把话题转移到其他领域，也许是更深的领域。&lt;/p>
&lt;p>“了解对方的情况”包含四个维度。&lt;/p>
&lt;ul>
&lt;li>第一个维度是，你的话是他们想听的，还是你想说的？&lt;/li>
&lt;li>第二个维度是，你的回应是否与对方处在同一个情绪层面上？&lt;/li>
&lt;li>第三个维度是，你看世界的角度与他们是否相同?(利亚姆想要就兰迪的事和他们公司的办公室氛围发泄情绪，而本却好奇到底是什么原因让利亚姆如此生气。)&lt;/li>
&lt;li>第四个维度是，你是不是没有对他人真正关心的事情做出回应?(利亚姆想要就兰迪的事和他们公司的办公室氛围发泄情绪，而本却好奇到底是什么原因让利亚姆如此生气。)&lt;/li>
&lt;/ul>
&lt;p>为了让对方听你说话，并且告诉你更多有关他们自己的事情，你需要让他们知道你在努力理解他们和他们的处境。一旦建立了这种联结，你们就有可能提出其他话题，并深入探索更多的问题。(在他们对话结束，前往吧台的时候，本意识到了这一点。尽管他想继续讨论下去，但他意识到利亚姆并不想。)寻找恰当的时机是“了解对方的情况”的另一个方面。本把这个问题放下了。不是所有事情都需要立即处理。&lt;/p>
&lt;p>保持好奇比看起来要复杂得多。对于一件事，你可能一无所知；你也可能自认为无所不知，提问只是为了验证自己的假设。后一种情况会带来一个问题，那就是你可能并非真的感到好奇。你在很大程度上已经产生了先入为主的见解，并且正在引导对方以证明你的观点。这种态度不太可能鼓励对方更加开放和坦诚。&lt;/p>
&lt;p>要确保你的好奇心是真诚的，最好的办法就是保持这样的心态：不管你认为自己多敏锐，多了解对方，实际上你并不真正了解对方面临的处境。这样能让你变得天真质朴。有了这种天真的好奇心，你就更可能问出一些鼓励自我表露的问题。&lt;/p>
&lt;p>&lt;strong>并非所有的问题都是一样的。询问正确的问题有助于鼓励别人分享。开放式问题能催生新选项、新视角，或者思考某种情况的新方法，进而扩大对话的范围&lt;/strong>（并非所有的问题都是一样的。询问正确的问题有助于鼓励别人分享。开放式问题能催生新选项、新视角，或者思考某种情况的新方法，进而扩大对话的范围）&lt;/p>
&lt;p>&lt;strong>最有效的开放式问题，通常都不是以“为什么”开头的。“为什么”式的问题往往会驱使人们去思考，而不是去感受。这类问题隐含着要对方为自己辩护的要求。&lt;/strong> 例如，如果本问“你为什么那么心烦”，利亚姆就会觉得他需要给出一个符合逻辑的解释。如果本继续追问“你为什么不干脆忘了兰迪的事”，利亚姆就不太可能透露他对办公室氛围的反感，以及对客观世界的向往了。在通常情况下，&lt;strong>符合逻辑的解释往往不能涵盖事情的所有方面。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>封闭式的问题通常能用“是”或“否”来回答。这种问题会缩小谈话的范围，更有可能给人被侵犯、被评判的感觉&lt;/strong>。例如本问利亚姆的这个问题——“你和同事聊过这些事吗”。&lt;/p>
&lt;p>**还有些问题是“伪问题”，这些问题其实是伪装为疑问句的陈述。这些问题同样是无效的。&lt;strong>如果本问“难道你不是因为嫉妒兰迪的说服力才生他的气吗”，这就是一个伪问题。&lt;/strong> 封闭式问题和伪问题听起来都很像伪装成问题的建议或对假设的验证。**本发现，即使对方寻求建议，建议也很少有用。**我们帮助对方的渴望，会使我们得出草率的结论。这些结论往往源于我们自身的经验，通常根本不适用于对方的情况。**我们很少能提出对方没有考虑过（很可能已经考虑并排除了）的选择。这些都是本落入的陷阱。&lt;/p>
&lt;p>&lt;strong>向他人提建议也会增加两人之间的权力差异。遇到问题的人可能在一开始就觉得低人一等，如果另一个人表现得好像自己知道答案，就可能会增加这种落差&lt;/strong>。提建议的另一个问题在于，**我们很容易误解对方真正想要的东西。**利亚姆也许说过他想要本的建议，可他真的想要吗？人们去找别人倾诉，可能有许多原因。也许他们想要一个把自己的想法说出来的机会。也许他们只想发泄情绪，寻找一双富有同情心的耳朵。有时他们只希望别人为他们遭受的不公正对待给予支持和共情，而不需要别人帮助他们想出解决方案。**倾听者需要清楚对方想要什么，然后才能弄清怎样才能提供最好的帮助。**有些时候，提问题的人，本身是带着答案来的，不需要提建议，只是需要一些空间而已。每个人都有自己的目标和实现目标的方法。人们提建议的时候，他们是从“他们会怎么做”的角度来说的，但没有充分考虑对对方来说最好的做法。**提建议还有一个弊端：你会看不到对方的真实情况。**对利亚姆来说，他如此生气，真的是因为兰迪抢了这个项目的功劳吗？还是因为他的办公环境有太多钩心斗角的事情？抑或是因为利亚姆希望世界是客观和理性的（也许这种希望并不现实）？**我们想要帮助别人的愿望，会让我们在发现真正问题之前就过早地着手解决问题。**有句格言说得很好：“用错误的方法解决正确的问题，好过用正确的方法解决错误的问题。”这样一来，你会更快地发现这是错误的解决方案。&lt;/p>
&lt;p>如果建议通常是没用的，那人们为什么还总是给人提建议？也许是因为解决别人的问题似乎比解决自己的问题容易得多。也许是因为我们希望有机会展示我们的分析能力。或者，也许是因为我们想成为力挽狂澜的英雄，收获他人的崇拜，留下锦囊妙计之后拂衣而去。不论是什么原因，&lt;strong>请问问自己：“我提建议是为了满足自己的需要，还是真的因为我想帮助对方？”&lt;/strong>&lt;/p>
&lt;p>如果你要给别人建议，你就必须充分了解情况，知道对方真正想要什么，并考虑对方的风格与做事方法。最重要的是，你必须把“我会怎么做”的想法放在一边。这些说起来容易，做起来难。此外，&lt;strong>提建议不一定有助于了解对方，只能了解他们对于你的建议的反应&lt;/strong>。&lt;/p>
&lt;p>&lt;strong>在帮助别人得到了解的过程中，你可能需要寻找支持他们的机会，以便他们能够更充分地表达自己的情绪。&lt;/strong> 你怎样才能知道，他们是否在谈及自己的感受时过于轻描淡写了？&lt;strong>从某种程度上说，你不可能知道，但你可以通过捕捉对方的语气、非言语信息，发现情况的严重程度与他们表达的情绪不相符等方面来推测。&lt;/strong> 本就注意到了利亚姆的焦躁。本强调说：“你听起来很恼火。”这句话鼓励了利亚姆更充分地表达他的愤怒。如果本能继续倾听并把利亚姆的感受用语言反映出来，倒有可能鼓励更多的自我表露，但不幸的是，本的做法恰恰相反。他问了一系列逻辑上的问题，让利亚姆脱离了感受，开始用理性做出反应，而利亚姆觉得这样毫无帮助。&lt;/p>
&lt;p>用语言将未表达的，或表达不充分的情绪反映出来，与询问有引导性的问题之间存在着微妙的差别。举个恰当的例子：“你说你有点儿烦恼，但听起来好像不是这样。你是不是其实很生气？”这句话可能属于上述任意一种类别。&lt;strong>区别在于你做出的假设，以及你提问时的语气。如果你承认自己永远不可能知道对方身上发生了什么，那么你就知道自己是在猜测。你的假设就是合乎分寸的——只是假设而已。提问不仅比陈述更为合乎分寸，还比你用确定的语气（就好像“你知道”一样）表达同样的意思更不容易招致对方的抵触。&lt;/strong> 你可以这样说，“听起来真让人沮丧”或者“如果这种事发生在我身上，我一定会生气的”。因为这些话反映了你心里的情绪，&lt;strong>这些是你的的确确知道的。这些共情的陈述更有可能鼓励对方充分表达自己的感受。&lt;/strong> 共情的陈述更有可能鼓励对方充分表达自己的感受。&lt;/p>
&lt;p>在第二次对话中，这两个人都承担了一些风险，而且收获了回报，让他们的友谊变得更加亲密了。如果本没有提及利亚姆改变话题的事情，或者逐渐疏远利亚姆（这两者都是常见的反应），他们的关系就会停留在较为肤浅的水平上。本告诉了利亚姆更多有关他对于他们友谊的希望，将自己置于脆弱的境地。在承认自己总是过于心急、逼得太紧时，他也是脆弱的。他为利亚姆的回答铺平了道路，让他也能够表露心迹。本发现自己“犯错”，以及随后利亚姆答应帮助本看到他自己何时逼得太紧，也都是很好的现象。他们采取了一些重要的行动。&lt;strong>他们依然需要重复并改进这个过程，轮流承担风险、袒露脆弱的一面，这样关系才会不断地向前发展。&lt;/strong> 不过，他们已经学会如何向彼此表露心迹，并开始体验到自我表露的好处了。&lt;/p>
&lt;p>好奇和唐突之间也有着十分细微的区别。如果你相信某个人真的想了解你，并且对方也告诉了你原因，你就可能觉得他的问题没有那么唐突。然而，如果你觉得对方把你当成了一个有意思的“标本”，把你放在“显微镜”下观察，你就远不会那么开放了。如果你不知道他会如何利用你告诉他的事情，那你就更不愿意分享了。&lt;/p>
&lt;p>相互性是自我表露的一个关键元素，但谁应该率先表露呢？虽说我们表露得越多，我们越能控制别人对我们的看法，但我们不得不考虑，一旦地位（或者对地位的感知）问题掺和进来，事情就会变得复杂得多。“地位”可能是组织中的层级、过去的成就，或者受教育水平。不幸的是，性别、种族和社会经济地位也会造成地位差异。要一个已经感到低人一等的人自我表露，这种要求实在太高了。此时自我表露会让人感觉风险更高，这是合乎逻辑的。然而，**地位较高的人往往不会意识到，他们的身份会让其他人难以向他们表露自我。**老板经常对直属下属说“我希望你们有话直说”，但他们却淡化了这样做的风险。&lt;strong>那些权力较大、地位较高的人不仅需要意识到这种关系动力，而且应该比与地位平等的人在一起时表露得更多。&lt;/strong>&lt;/p>
&lt;p>我们在此做了一个假设：人们想要更深入地了解他人。也许你对此感到有些矛盾。你真的想要他人把童年的创伤，或者他们与伴侣的关系问题一五一十地讲给你听吗？这样的了解又会让你背负什么样的义务呢？你是否需要答应他们的请求，或者承诺站在他们那一边？你能否始终给予他们想要的那种深度关系？&lt;/p>
&lt;p>自我表露也可能导致棘手或尴尬的后果。随着关系的发展，人们对彼此的期望也越来越高。虽然帮助他人、回应他人的需求是建立稳固关系的重要部分，但设置边界也至关重要。在关系的发展过程中，不同的时期可能需要不同的边界。当边界问题产生时，你必须发现、提出并成功解决这些问题。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>深化学习&lt;/strong>&lt;/p>
&lt;p>&lt;strong>自我反思&lt;/strong>&lt;/p>
&lt;p>&lt;strong>1. 把自己放在本的位置上。&lt;/strong>
你想和利亚姆建立更亲密的私人关系。你喜欢他，也看重他的分析能力，但希望他能多分享一些有关自己的事情。你很容易自我表露，也希望利亚姆多一些自我表露。你可以做些什么来鼓励他这样做？&lt;/p>
&lt;ul>
&lt;li>请回忆一下本章描述的各种情况，你认为你可能会怎么做？你是否认为自己在某些方面的交流不够有效？&lt;/li>
&lt;li>这说明，你在帮助他人更多自我表露方面的能力（和风格）如何？&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>&lt;strong>2. 选择一段你之前找出的重要关系。要更充分地了解对方，你会采用下面哪种方法？&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>积极倾听，尝试充分理解对方。&lt;/li>
&lt;li>放下评判，不试图过快地弄清对方发生了什么事。&lt;/li>
&lt;li>对于对方所看重的事物保持好奇和探索精神。&lt;/li>
&lt;li>使用开放式问题来鼓励对方分享更多。&lt;/li>
&lt;li>倾听对方的情绪，并帮助他们充分表达（例如“你听起来不只是有点儿烦恼，你现在感觉如何”）。&lt;/li>
&lt;li>共情，尤其是对情绪表达共情（“听起来真让人沮丧”）。&lt;/li>
&lt;li>表达接纳（“我真的能理解你为什么会有那样的反应”）。&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>&lt;strong>3. 反之，你是否倾向于做下面的事情？这些事情会让对方不愿意分享，也不利于你更充分地了解对方。&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>倾听时心不在焉，因为你已经在考虑（或者已经决定）如何做出回应。&lt;/li>
&lt;li>很快就改变话题，去谈论自己或自己感兴趣的话题。&lt;/li>
&lt;li>认为你已经弄清对方真正的问题。&lt;/li>
&lt;li>询问引导性的问题，让对方接受你的结论。&lt;/li>
&lt;li>忽视对方的感受，用逻辑来证明你的观点。&lt;/li>
&lt;li>评判对方的言行。&lt;/li>
&lt;li>不能对他们的处境感同身受。&lt;/li>
&lt;/ul>
&lt;p>你为什么常会用这些方式回应对方？&lt;/p>
&lt;hr>
&lt;p>&lt;strong>应用&lt;/strong>&lt;/p>
&lt;p>你在上一部分回答的问题反映了你对自己的看法。这段关系里的另一方也这么看你吗？去问问他们，你的哪些行为支持或者限制了他们的自我表露意愿。&lt;/p>
&lt;p>提醒他们，你之所以提出这些问题，是因为你想让你们的关系变得更加开放。你在使用你的自我表露来鼓励他们的自我表露，并借此深化你们的关系。因此，这是为了增进私人关系，而不是学术探讨。&lt;/p>
&lt;hr>
&lt;p>&lt;strong>理解&lt;/strong>&lt;/p>
&lt;p>如果你完成了“应用”里的练习，你就会了解到他人的看法在多大程度上与你一致。对你来说，倾听他人的看法是否容易，尤其是在他们的观点与你不同的情况下？&lt;/p>
&lt;p>请想想你找出的其他重要关系。你现在能做些什么，来鼓励那些人更好地被你了解？&lt;/p>
&lt;p>在这个过程中，有一个内在的困境。你在一边学习新的行为，一边建立关系，但你不希望关系中的对方觉得他们被利用了，或者在你的实验中充当了实验对象。关于如何做到这一点，你学到了什么？顺便问一句，你有没有问过他们对此的感受？&lt;/p>
&lt;/blockquote></description><category domain="https://yinyajun.github.io/categories/%E6%83%85%E6%84%9F/">情感</category><category domain="https://yinyajun.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%85%B3%E7%B3%BB/">深度关系</category></item><item><title>《深度关系》金句摘录-第三章</title><link>https://yinyajun.github.io/posts/emotion/deep-relation03/</link><guid isPermaLink="true">https://yinyajun.github.io/posts/emotion/deep-relation03/</guid><pubDate>Tue, 06 Jan 2026 06:20:10 +0000</pubDate><author>skyblueice234@gmail.com (雅俊)</author><description>&lt;p>不知道怎么建立这种联结？自我表露可以创造更多的联结机会，增加信任。&lt;/p>
&lt;p>害怕自我表露导致评判和拒绝，这样的恐惧非常真实。&lt;/p>
&lt;p>如果有人知道导致你做出某种行为的所有原因，他们就更可能原谅起初看起来过分的行为。&lt;/p>
&lt;p>没有自我表露，就不可能建立更深的关系&lt;/p>
&lt;p>15%法则：原理舒适区，踏入学习区的15%&lt;/p>
&lt;ul>
&lt;li>这是主观的，要对合适的对象&lt;/li>
&lt;li>对别人的影响&lt;/li>
&lt;li>注意场合&lt;/li>
&lt;/ul>
&lt;p>不断走出自己舒适区的过程也是建立关系的关键，这是不断自我表露的基础。&lt;/p>
&lt;p>分享感受往往是更有影响力的做法。&lt;/p>
&lt;p>情绪的好处是为事实赋予意义，情绪为交流提供了色彩，以一种完全不同于平静和理性的方式吸引他人。&lt;/p>
&lt;p>好的交流中，我们必须既表达想法（认知）和感受（情绪）&lt;/p>
&lt;p>分享困境的正反两面，可以更完整的表达这个问题。&lt;/p>
&lt;ul>
&lt;li>什么事对你很重要&lt;/li>
&lt;li>什么事情让你陷入困境&lt;/li>
&lt;li>表露会展示脆弱，这是深化联结要付出的代价&lt;/li>
&lt;/ul>
&lt;p>“我觉得...”&lt;/p>
&lt;ul>
&lt;li>
&lt;p>如果你把“觉得”或“觉得好像”换成“认为”，句子依然说得通，那你就不是在表达情绪。例如，“我觉得你想当老大”和“我认为你想当老大”表达了同样的意思——因为两者表达的都是认知，不是感受。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>请想一想“我觉得烦躁、被忽视”和“我觉得你不关心我”这两句话的区别。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&amp;quot;我感受到&amp;quot;来表达一种情绪，“我认为”来表达认知&lt;/p>
&lt;/li>
&lt;li>
&lt;p>我觉得烦躁、被忽视”是一句关于“我”的陈述，而“我觉得你不关心我”是一句指责，很可能导致对方的防御。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>你想要给别人留下特定的印象，这可能会让你感到选择受限，但你仍有选择的余地&lt;/p>
&lt;ul>
&lt;li>
&lt;p>你可能会受他人反应的影响，但是否接受影响也是一种选择&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对方在沟通中的反应，可能会让自己做出自我表露的决定变得更容易，或者更难，但表露与否归根结底是自己的决定&lt;/p>
&lt;/li>
&lt;li>
&lt;p>这种能在现实中采取行动的信念叫做能动性（对于发生在自己身上的事情做出何种反应，人们认为自己是没有选择余地的）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>这些方法能帮助我们拥有更多的能动性，让我们比自己最初所想的那样拥有更强的影响力。这是一种重要的心态，因为要发展更深刻、更有意义、更有深度的关系，就要做出有挑战性的选择。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>我们对于选择、影响力及能动性的强调，并不意味着你可以仅靠自己建立深度关系。关系是由双方共同决定的&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>当你不确定自己的表露会带来什么影响的时候，你就会觉得有风险，这也是你感到最脆弱的时候。如果你在相似的场合下多次表露过相同的事情（即使这件事很私密），而且你很清楚别人会有什么反应，即使那种反应是消极的，你也不会感到很脆弱，远远没有你分享一些从没对别人说起过的事情时那样脆弱。&lt;/p>
&lt;p>当人不知道自己会被如何评价时，最容易失去自我稳定感；而这种不稳定，会让人本能地向他人靠拢，寻求确认与依附。&lt;/p>
&lt;p>自我表露需要坚毅的品质和内在的力量&lt;/p>
&lt;p>当他们卸下面具、冒险表露真心的时候，却发现生活中的其他人反而认为他们更坚强、更值得（也可能更不值得）信任了，这让他们感到非常欣慰。&lt;/p>
&lt;p>&lt;strong>即使是安全、平常的话语，也不是完全没有风险的。在缺乏信息的情况下，人们会编造出一些东西。每个人都会在与人交往时得出一些结论。我们表露得越少，他人就会编造更多的信息，来填补认知的空白，以便理解他们看见的我们。如果我们太过谨慎，不轻易表露自己的感受，我们实际上就更加无法控制别人对我们的看法了。&lt;/strong>&lt;/p>
&lt;p>如果我们只分享自己的一面，就会产生另一种形式的沉默——在那种情况下，对方看不到也不了解真实的我们，包括那些更有趣的部分。即使我们成功地树立了某种形象，那也只是一个毫无意义的胜利。&lt;strong>这只证明了真正的自我不受人欢迎而已&lt;/strong>&lt;/p>
&lt;p>同样糟糕的是，一旦我们给别人留下了某种固定的印象，我们往往就会一直如此表现，而真实的自我会越来越不为人所知。这样做的代价是更加孤独，以及我们所说的“渐渐地被秘密束缚住了”。我们自身重要的部分，通常与我们真实自我的其他方面有关。隐藏自我的某一部分，可能会让我们隐藏更多的东西，导致我们表现出来的部分越来越少，最终让我们的关系变得越来越肤浅。&lt;/p>
&lt;p>“在我知道我能信任那个人，得到他的接纳之前，我不会冒险透露很多信息。我需要先知道他会有什么反应”。我们认为，这里的因果关系应该颠倒过来——承担15%的表露风险是建立安全感的基础。&lt;strong>如果每个人都在等着对方去冒险，关系就不会有发展。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>在不知道结果的情况下冒险，是建立深刻的人际关系的关键&lt;/strong>。在这个过程中，你必须顺其自然，相信从长期来看，通过率先自我表露，你更有可能建立信任、获得接纳，得到你最想要的关系。这就是“拥有能动性”的关键所在。&lt;/p>
&lt;blockquote>
&lt;h3 id="练习">练习&lt;/h3>
&lt;h4 id="自我反思">自我反思&lt;/h4>
&lt;p>1.把自己放在埃琳娜的位置上。在上一份工作里被解雇的经历依然让你隐隐作痛，你最不想要的结果就是让同样的事再次发生。一部分的问题在于，你当时不了解公司的文化。你喜欢桑杰，喜欢和他一起在工作组里合作的经历。他是那个能让你坦诚相待的人吗？他是能帮助你学习如何在这家公司取得成功的人吗？你想要一段真诚的关系，让你能够分享重要的事情，但他会对你产生错误的消极看法吗？&lt;/p>
&lt;p>● 你认为，要让别人充分地了解自己，你会怎么做？请想一想埃琳娜在这一章中面临选择的几个时刻——如果是你，你会怎么说？要让别人在这种工作场合下充分了解自己，对你来说你有多容易（或困难）？你的选择说明了什么？&lt;/p>
&lt;p>2.让别人了解你：一般而言，让别人知道你在乎什么，对你来说容易吗？你觉得最难以分享的话题是什么？对于分享那些话题，你的担忧是什么？&lt;/p>
&lt;p>3.重要的人际关系：从你在上一章里找出的重要人物中选出一个人。有没有一些与你、与那段关系相关的事情，你没有完全与那个人分享？透露这些事情会让你担心吗？&lt;/p>
&lt;p>4.表露情绪：分享你的情绪对你来说有多简单（困难）？请查看附录A里面的情绪词汇。对你来说，是否有些感受比其他感受更难以向别人表达？&lt;/p>
&lt;h4 id="应用">应用&lt;/h4>
&lt;p>在上文的问题3里，你提出了一些与某段关系有关的问题。如果要你走到舒适区外15%的位置，你会分享什么？&lt;/p>
&lt;p>注意，潜在的自我表露包含两个方面。第一个方面是内容，第二个方面是你对于分享这些内容的感受和担忧。埃琳娜在自我表露中透露了这两个方面的信息。如果你按照上文的要求，与那个人进行了对话，那么你在多大程度上分享了这两个方面？&lt;/p>
&lt;p>在接下来的一周里，在与朋友和熟人的对话中，请试着拓宽你自我表露的范围，分享一些在你舒适区之外的事情，一些你通常可能不会分享的事情。这些事情可以是事实、观点或者感受。&lt;/p>
&lt;h4 id="理解">理解&lt;/h4>
&lt;p>在与你选择的那个人对话时，你从你的自我表露中学到了什么？在自我反思部分，你思考了你对表露可能怀有的担忧。那些担忧有什么改变？你在这个过程中对自己有什么新的了解？这种了解如何影响了那段关系？&lt;/p>
&lt;p>对于在与他人的互动中分享更多的私人话题，你有什么感想？你在当时有什么情绪？这样做对那些互动的性质有没有影响？&lt;/p>
&lt;p>在未来的互动中，你会如何运用所学的东西？（请具体说明你会对不同的人做什么。）&lt;/p>
&lt;/blockquote></description><category domain="https://yinyajun.github.io/categories/%E6%83%85%E6%84%9F/">情感</category><category domain="https://yinyajun.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%85%B3%E7%B3%BB/">深度关系</category></item><item><title>源远流长的希腊神话传说</title><link>https://yinyajun.github.io/posts/history/greek02/</link><guid isPermaLink="true">https://yinyajun.github.io/posts/history/greek02/</guid><pubDate>Thu, 04 Jul 2019 21:57:10 +0000</pubDate><author>skyblueice234@gmail.com (雅俊)</author><description>&lt;h2 id="希腊神话的源流">希腊神话的源流&lt;/h2>
&lt;p>在克里特文明时期，已经有了流传甚广的神话传说，有的随着克里特文明的毁灭而消失，有的则在迈锡尼时期与阿卡亚人的神话传说相融合。&lt;/p>
&lt;h3 id="克里特时期">克里特时期&lt;/h3>
&lt;p>带有浓重的埃及成分，埃及的迷宫、半人半兽的神灵都带有&lt;strong>神秘色彩&lt;/strong>，&lt;strong>能工巧匠&lt;/strong>辈出，像木乃伊，以现代的科技手段依然难以复制，克里特也带有这样的色彩（精巧和扑朔迷离）。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>米诺陶诺斯神话&lt;/strong>：传说米诺斯国王在王宫中间养了一头怪牛，是他的妻子与牛所生，每9年向雅典索要7对童男童女来喂养，第三次进贡（置于为什么进贡，可以查看忒休斯的生平故事）的时候有一个少年忒休斯来到王宫，与米诺斯的女儿阿里阿德涅相爱。女儿为了帮助他给他一把剑和一段绳索，忒休斯来到迷宫前，将绳索一头系在门口，深入迷宫杀死怪牛又出来。后成为成语“阿里阿德涅之线”（Ariadne's thread，比喻走出迷宫的方法和路径，解决复杂问题的线索。）。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>代达罗斯神话&lt;/strong>：代达罗斯是迷宫的修建者，米诺斯为了长期差遣代达罗斯就将他囚禁在王宫里。代达罗斯为了将儿子伊卡洛斯送出，建造了一对翅膀，用蜡粘在身上。飞出后，很骄傲，欲与天公试比高，飞近了太阳，太阳神阿波罗将蜡融化，翅膀掉落，儿子就掉海里摔死了。
值得一提的是，IceFrog将Dota1中的装备“暴雪弩炮”，在Dota2中改名为“代达罗斯之殇”，而凤凰的名字也叫“伊卡洛斯”，其中深意可以想象。&lt;/p>
&lt;/blockquote>
&lt;h3 id="迈锡尼神话">迈锡尼神话&lt;/h3>
&lt;p>阿卡亚人在入侵伯罗奔尼撒岛之前，曾居住在马其顿，周围最高的山叫奥林匹斯山，因为神的至高无上的地位，他们开始崇拜奥林匹斯山。奥林匹斯神族中的那些作为征服者的神（宙斯、阿瑞斯、阿波罗、阿尔忒弥斯、赫尔墨斯等）和英雄（阿伽门农、赫拉克勒斯等）随着印欧语入侵者阿卡亚人一同进入希腊半岛。&lt;/p>
&lt;p>闪米特语系崇拜女神，而印欧语系崇拜男神（同样出现在其他印欧语系入侵者的神话中，比如北欧神话）。所以阿卡亚人崇拜的神以男性为主（主神宙斯），崇拜英雄，崇拜战争，这和克里特神话中精巧的特性不太一样。&lt;/p>
&lt;p>阿卡亚人崇拜的神是同人同性的，甚至比人更人，为人之典范，其为神并不因智慧，而是粗犷的形体之美。而经过埃及和克里特文明流传的半人半兽形态的神尽管也有保留，但是大都成为了反面形象，例如地狱三头狗、头发都是蛇看谁就成石头的美杜莎、用歌声催眠的塞壬女妖以及狮身人面的斯芬克斯等。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>赫拉克勒斯Hercules&lt;/strong> ：大力士，是主神宙斯与阿尔克墨涅之子。干了12件惊天动地的大事，被多利亚人奉为先祖，曾活剥一头最凶悍的狮子，披在身上。
&lt;strong>雅典娜Athena&lt;/strong>：智慧女神、女战神，全身披挂，威风凛凛。
&lt;strong>维纳斯Venus&lt;/strong>：美神 阿佛洛狄特Aphrodite
&lt;strong>阿喀琉斯Achilles&lt;/strong>：特洛伊战争主角，海洋女神忒提斯（Thetis）和英雄珀琉斯（Peleus）之子
&lt;strong>阿伽门农Agamemnon&lt;/strong>：迈锡尼国王，希腊联合远征军统帅&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Coldplay单曲《&lt;strong>Something Just Like This&lt;/strong>》中的歌词：
Achilles and his gold
Hercules and his gifts&lt;/p>
&lt;/blockquote>
&lt;h3 id="地理梳理">地理梳理&lt;/h3>
&lt;p>希腊本土主要为两个半岛组成，一个是伯罗奔尼撒半岛，以中央的伯罗奔尼撒平原为名，另一个是阿提卡半岛。
前者有许多重要的城邦，比如迈锡尼和后来的斯巴达，还有奥林匹亚，奥林匹亚是每隔四年祭奠奥林匹斯山诸神的地方，即后来的奥林匹亚竞技会，公元前776年为第一届。后者有今天希腊的首都雅典、底比斯、马拉松。再往北，有城邦德尔斐，建有祭奠阿波罗的神庙，因其神谕灵验，在希腊人心中地位神圣。&lt;/p>
&lt;h3 id="希腊神话源流小结">希腊神话源流小结&lt;/h3>
&lt;p>荷马史诗中的特洛伊战争，赫西俄德神谱中奥林匹斯神族和泰坦神族的战斗，这些神话传说反映了了希腊入侵者（阿卡亚人、多利亚人等）对爱琴海世界的血与火的征服活动。美轮美奂的希腊神话传说是历史融合的结果，既包含埃及、巴比伦等神话的神秘色彩，又融入了北方入侵者的粗犷风格。&lt;/p>
&lt;ol>
&lt;li>埃及宗教的影响（精巧、扑朔迷离、半人半兽）&lt;/li>
&lt;li>北方游牧民族神话的影响（战神、征服者，英雄）&lt;/li>
&lt;li>巴比伦神话的影响（巴比伦史诗&lt;em>恩努马-艾利希Enuma Elish&lt;/em>，谱系分明，又称《神谱》）&lt;/li>
&lt;/ol>
&lt;h2 id="荷马史诗和系统叙述诗">荷马史诗和系统叙述诗&lt;/h2>
&lt;p>神话的融合虽然是悄无声息的，或者说通过激烈的战争碰撞。在这种无意识的交融过程中，荷马和赫西俄德以及大量的佚名诗人起到了穿针引线的作用。更重要的是，在黑暗时代，将古老的克里特、迈锡尼神话传说传递给后来的希腊城邦时代。&lt;/p>
&lt;h3 id="荷马背景">荷马背景&lt;/h3>
&lt;p>生活在公元前九世纪末八世纪初，即黑暗时代即将过去，城邦时代即将来临的时期。他双目失明，像他的许多前辈一样游吟传唱，所唱的是个个不同的小故事，但是却将希腊的诸神系统地联系起来，形成波澜壮阔、内容庞杂的百科全书式的史诗。&lt;/p>
&lt;h3 id="史诗背景">史诗背景&lt;/h3>
&lt;p>&lt;strong>以迈锡尼时期的英雄为主角&lt;/strong>，讲述的是神和英雄的故事及他们的血缘关系。英雄是半神，是神和凡人的子嗣（hero，半神）。两部史诗所记载的故事都是对迈锡尼时期曾经发生过的战争和航海活动的一种神话化的渲染，是对阿卡亚人以往的英雄业绩的赞美讴歌。&lt;/p>
&lt;h3 id="伊利亚特">《伊利亚特》&lt;/h3>
&lt;p>主要内容是特洛伊战争，希腊英雄或阿卡亚英雄与特洛伊英雄的战争，因双方与神灵有血缘关系，所以神也加入了战争。&lt;/p>
&lt;ol>
&lt;li>战争背景：伊利亚特，即为特洛伊，是迈锡尼东北角的小亚细亚的城邦。特洛伊有一个王子帕里斯，出访斯巴达，看上了斯巴达国王墨涅拉奥斯的妻子海伦，在爱神阿佛洛狄忒的帮助下，诱拐了海伦。墨涅拉俄斯十分愤怒，找到了自己的哥哥迈锡尼城邦的国王阿伽门农。阿伽门农号令周围的希腊城邦攻打特洛伊，运用奥德修斯的木马计把城门撞开，抢回海伦，凯旋而归。&lt;/li>
&lt;li>作品内容：&lt;strong>并没详述整个来龙去脉&lt;/strong>，是从战争第十年后发生的一个插曲（阿喀琉斯的愤怒）。希腊联军第一勇士阿喀琉斯在战场上获得一个女俘虏，之后与阿伽门农发生冲突，阿伽门农要将女俘虏占为己有。阿喀琉斯一怒之下退出战争，希腊联军面对以大王子赫克托耳（特洛伊第一勇士，特洛伊方的统帅）为首的特洛伊军队节节败退。阿喀琉斯的好朋友特洛克罗斯盗用了阿喀琉斯的盔甲假扮他出战，被赫克托耳杀死阵前。悲痛愤怒的阿喀琉斯重新出战，击杀了不可战胜的赫克托耳。最后特洛伊国王普里阿摩斯劝说阿喀琉斯动恻隐之心，求得了儿子尸体，特洛伊人民为其举行了盛大的葬礼。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>《伊利亚特》（Iliad）：
犹如凶煞星忽然从浓云里呈现出来，
光亮闪烁，忽而又隐进昏暗的云层里；
赫克托耳也这样一会儿出现在队伍的最前列，
一会儿有隐进队伍的后列里，向他们训令。
他一身铜装，犹如提大盾的父神宙斯的闪电……&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Greek是罗马占领希腊后的称呼，希腊人自称为Hellas，即海伦&lt;/p>
&lt;/blockquote>
&lt;h3 id="奥德修纪">《奥德修纪》&lt;/h3>
&lt;p>特洛伊战争之后英雄回乡的故事。全文采用了倒叙的手法，奥德修斯在归乡途中的第十年来到了一个岛，向岛上的国王讲述了他这十年中经历的逸闻轶事。&lt;/p>
&lt;blockquote>
&lt;p>其中的食荷国的莲蓬，吃了会使人忘记家乡。
风神送给他们一个口袋，可以把所有的逆风都装进去，这样便能一帆风顺回家了。不料当船快行驶到家时，众水手以为口袋里面装的是金银财宝，乘奥德修斯睡觉时打开了口袋，结果各路风神倾刻呼啸而至，又把他们吹到风神岛。
奥德修斯还用烧红的木棍战胜了独目巨人，藏在羊的肚子底下出逃。
来到女妖克尔克的岛上，克尔克将他的同伴变成了猪。在智慧女神的帮助下，战胜了女妖。
游历了地狱，看到了阿喀琉斯的幽灵。
途中遇到塞壬女妖，她用美妙的歌声引诱水手触礁，让水手们用蜡堵住耳朵，并命人把自己绑在桅杆上，最终化险为夷。
在日神岛上，同伴宰食神牛，激怒宙斯，被雷电劈毁船只。
冲到卡吕普索的岛上，并且被软禁了七年。&lt;/p>
&lt;/blockquote>
&lt;p>最后岛主被故事打动，帮助他回到家乡伊大卡岛。回到家乡后发现有许多纨绔子弟觊觎他的王位与坚贞的妻子珀涅罗珀，奥德修斯与儿子忒勒马科斯通过种种计谋杀死心怀不轨的人，一家团圆。&lt;/p>
&lt;h3 id="系统叙事诗">系统叙事诗&lt;/h3>
&lt;p>以神话和英雄传说为内容，填补了荷马史诗和赫西俄德神谱没有提及的地方，将分散凌乱的希腊民间神话传说联系为彼此相关的有机体系。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>《诶塞俄比亚英雄》阿喀琉斯之死&lt;/strong>：阿喀琉斯的母亲是海洋女神，他出生时母亲倒提着他在冥河里泡过，全身刀枪不入，唯一的弱点是后脚跟。最后被花花公子帕里斯从背后偷袭，射中了后脚跟而死。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>《塞浦路斯之歌》三女神的金苹果之争和特洛伊战争的起源&lt;/strong>：阿喀琉斯的母亲海洋女神特提斯与其父亲特萨利亚国王珀琉斯结婚时，请来各路英豪来观礼，却唯独忘了纷争女神。愤怒的纷争女神就在宴会上投下了一个金苹果，上面写着：献给最美丽的女神。与会的女神都认为自己最美，其中就有全身雪白的赫拉、智慧女神女战神雅典娜、美神阿佛洛狄特，三人尤其不甘心，就找上宙斯。宙斯是雅典娜和维纳斯的父亲，赫拉的丈夫，就建议她们出城去找放牧的少年帕里斯评判。 帕里斯因其在出生时母亲梦到特洛伊有血光之灾而被抛弃，三个女神找上他，分别许以最大的城邦、最强大的力量、最美的女人，帕里斯选择了美女。后来美神就让国王重新召回儿子，并引导帕伊斯来到斯巴达抢走海伦，由此引发特洛伊战争。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>《小伊利亚特》《特洛伊失陷记》木马计&lt;/strong>：特洛伊守城十年，希腊联军久攻不下，奥德修斯于是想出了木马计。他命令联军佯装撤退，留下巨大的中空的装有希腊战士的木马在海滩上，特洛伊军队以为自己守城胜利，将木马作为战利品带回城中欢庆。等到夜深人静，木马肚中的战士与重返的联军里应外合，攻下了特洛伊。&lt;/p>
&lt;/blockquote>
&lt;h2 id="赫西俄德和神谱">赫西俄德和《神谱》&lt;/h2>
&lt;h3 id="神谱地位与意义">《神谱》地位与意义&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>独特贡献
与荷马史诗偏重故事性和英雄传说相比，赫西俄德通过游吟，讲述、理清了神的谱系，即神与神的血缘关系，使神的来历变得更加清楚，形成系统。希腊人眼里，神与神的后裔是神，神与人的后裔是英雄，这些英雄往往又是贵族家族的始祖，从家族谱系到英雄谱系，再到诸神的谱系。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>文化学意义
（1）通过神系的生殖原则反映了一种朴素的宇宙起源论和自然演化观，即在科学与哲学产生之前的希腊超越道德的富有童趣的对自然与社会的看法。最早的大地之神、天宇之神代表最原始的自然观，后来出现了美神、商神、战神反应了自然到社会的演变。
（2）蕴含着一种以“命运”为动力的社会进化思想。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="神谱谱系">《神谱》谱系&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>混沌（chaos）中产生了大地盖亚、爱神艾洛斯、地狱塔尔塔罗斯、黑暗和黑夜、白天和光明；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>未经交配，盖亚生下了与她一样宽广的天宇之神乌兰诺斯与海神蓬托斯；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>盖亚与乌兰诺斯相结合，生下了顶天立地的一批神族泰坦神族与巨人族；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>盖亚与乌兰诺斯最小的儿子克罗诺斯与姐姐瑞亚相结合，生下了奥林匹斯诸神；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>奥林匹斯诸神包括：1）克洛诺斯与瑞亚所生的三男三女，农神德墨忒尔、赫拉、灶神赫斯提亚（女）、海神波塞冬、地狱之神普路同（哈德斯）、宙斯；2）宙斯的后代：太阳神阿波罗、战神阿瑞斯、雅典娜、阿佛洛狄特等等&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>泰坦神族&lt;/strong>：泰坦即高大，也有神经质的意思。乌兰诺斯得知他的儿子中将会有一人代替他，所以将所生十二个儿子都打到地狱里去，最小的一个儿子克洛诺斯起来反抗，在盖亚的庇护下阉割了乌兰诺斯，解放了兄弟姐妹。（泰坦尼克号就是大的意思）&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ol>
&lt;h3 id="神谱悲剧">《神谱》悲剧&lt;/h3>
&lt;ol>
&lt;li>乌兰诺斯的悲剧&lt;/li>
&lt;li>克洛诺斯的悲剧：克洛诺斯得到了和乌兰诺斯一样的提示，于是将自己的所有子女在出生时就吞食掉，瑞亚为了躲避他这种蛮横的行为，通过盖亚的帮助躲在小岛上生下了宙斯，将装有石头的襁褓给了克洛诺斯吞食。宙斯长大后变得更加强大，找到了一种神药让克洛诺斯呕吐，将兄弟姐妹都呕吐出来，由此发生泰坦神族占据南方山头与在北方的奥林匹斯神族的战斗，即北方民族与南方民族的战斗。&lt;/li>
&lt;/ol>
&lt;h3 id="神谱后续">《神谱》后续&lt;/h3>
&lt;ol>
&lt;li>《普罗米修斯三部曲》：希腊悲剧之父埃斯库罗斯作先知普罗米修斯三部曲，第一部为《被缚的普罗米修斯》，后两部亡佚，内容保存。&lt;/li>
&lt;li>《被缚的普罗米修斯》：宙斯获胜后同样获得提示，但是他并没有像前代统治者一样采用残暴的方式，而是依然和平相处。这被后世哲学家们分析为专制社会向民主的过度。唯一知道的是先知普罗米修斯，宙斯将他捆绑在高加索的山上，白天要恶鹰啄食肝脏，晚上又修复。&lt;/li>
&lt;li>亡佚内容：普罗米修斯妥协，告知宙斯与海洋女神忒提斯的子女将不可战胜，并且海洋女神与任何神的结合都将不可战胜，于是宙斯将忒提斯强行嫁给英雄珀琉斯，生出的小孩即是阿喀琉斯。(普罗米修斯的妥协即是诶斯库罗斯的妥协。 )&lt;/li>
&lt;/ol>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>荷马史诗、系统叙事诗和《神谱》不仅把散乱的希腊神话和英雄传说连缀为一个有机的体系，而且把某些英雄的家族历史和不幸遭遇当做叙述的主题，开始突出命运和神谕的重要性，从而具有了最初的悲剧色彩，使后来的人们开始关注背后的命运，引出了形而上的&lt;strong>希腊哲学&lt;/strong>。&lt;/p></description><category domain="https://yinyajun.github.io/categories/%E5%8E%86%E5%8F%B2/">历史</category><category domain="https://yinyajun.github.io/tags/%E5%8F%A4%E5%B8%8C%E8%85%8A/">古希腊</category></item><item><title>爱琴文明-从克里特到迈锡尼</title><link>https://yinyajun.github.io/posts/history/greek01/</link><guid isPermaLink="true">https://yinyajun.github.io/posts/history/greek01/</guid><pubDate>Wed, 03 Jul 2019 21:57:10 +0000</pubDate><author>skyblueice234@gmail.com (雅俊)</author><description>&lt;h2 id="希腊文化">希腊文化&lt;/h2>
&lt;p>希腊文化是西方文化的摇篮，而希腊文化本身也经历了漫长的发展。从希腊文化的开端克里特文明到希腊城邦文明再到希腊化时期文明也经历了2000多年时间。现在所说的希腊文化一般特指希腊城邦文明。在希腊城邦文明之前还有一个更为古老的&lt;strong>爱琴文明&lt;/strong>，它分为两个阶段：&lt;strong>克里特文明&lt;/strong>和&lt;strong>迈锡尼文明&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>克里特文明（源头在南面克里特岛）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>迈锡尼文明（希腊本土东北角的一个城邦）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>爱琴海文明&lt;/p>
&lt;/li>
&lt;li>
&lt;p>希腊城邦文明&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="克里特文明">克里特文明&lt;/h2>
&lt;p>爱琴文明第一个阶段，西方文化发源地。&lt;/p>
&lt;h3 id="一克里特岛地理位置">一、克里特岛地理位置&lt;/h3>
&lt;p>位于地中海东部中央，离东部小亚细亚（土耳其境内），南埃及，西亚（腓尼基，耶路撒冷），北巴尔干半岛（希腊本土）距离大致相等。&lt;/p>
&lt;p>而希腊位于巴尔干半岛，东边为地中海中的爱琴海，西边是亚平宁半岛（意大利半岛，亚德里亚海），再西边是伊比利亚半岛（西班牙半岛）。&lt;/p>
&lt;h3 id="二克里特文明发展">二、克里特文明发展&lt;/h3>
&lt;p>在公元前2500左右，由于受埃及文明的影响和少许两河流域文明影响，开始出现文字。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>两河流域文明&lt;/strong>，又叫美索不达米亚文明，是指在底格里斯河和幼发拉底河两河流域之间的美索不达米亚平原（现今伊拉克境内）所发展出来的文明，是人类最早的文明。尼罗河流域的埃及文明对克里特文明影响较大，克里特人也比较接近埃及人。&lt;/p>
&lt;/blockquote>
&lt;p>在公元前19世纪时，已有将近十万人居住，十分繁荣，又接受希腊本土文明（北方游牧民族）的影响，文明大放异彩；&lt;/p>
&lt;p>在公元前17世纪时，出现了以克诺索斯（克里特岛的中心）为首都的王国，被称为米诺斯王国，进入鼎盛时期。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>米诺斯王朝故事&lt;/strong>：相传住在奥林匹斯山的希腊神话主神宙斯来到了西亚，看上了一个推罗国王的女儿欧罗巴。由于妻子赫拉善妒，宙斯变为一只公牛驮着欧罗巴来到了克里特，生下的儿子建立了米诺斯王国。后来整个欧洲以欧罗巴起名的。&lt;/p>
&lt;/blockquote>
&lt;p>由伊文思考古所得：米诺斯迷宫、精美壁画、“逗牛”传统（由埃及传入到克里特到希腊到罗马，现成为西班牙的独特传统）、1600块线性文字A的泥板，至今无法破译&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>米诺斯王宫的来源&lt;/strong>:克里特岛的国王米诺斯，得罪了海神波塞冬，海神就降下了一头神牛，让神牛与米诺斯的妻子私通，生下了一个人身牛头的怪物米诺陶诺斯。这让米诺斯丢尽了脸。为了遮丑，他请代达罗斯修建了一座迷宫，把这个牛头怪关在里面，不让它出来，也不让别人进去。可是，这头怪物每隔9年就要吃7对童男童女，由当时臣服于米诺斯的希腊城邦雅典进贡。&lt;/p>
&lt;/blockquote>
&lt;p>公元前1500,米诺斯文明毁灭，原因不明。普遍认为是：&lt;/p>
&lt;ol>
&lt;li>地质灾害：火山爆发。&lt;/li>
&lt;li>人祸：来自北方的阿卡亚人（印欧语系，而克里特人被认为是属于闪米特语系）的暴力入侵。&lt;/li>
&lt;/ol>
&lt;h2 id="迈锡尼文明">迈锡尼文明&lt;/h2>
&lt;p>爱琴文明第二个阶段。&lt;/p>
&lt;h3 id="一地理位置">一、地理位置&lt;/h3>
&lt;p>位于巴尔干半岛南部的伯罗奔尼撒半岛（像树叶）中央。&lt;/p>
&lt;h3 id="二文明发展">二、文明发展&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>文明发掘：由现代考古学之父施里曼发掘&lt;/p>
&lt;/li>
&lt;li>
&lt;p>风格特点：阿卡亚人（Achaean）在公元前1500毁灭了克里特文明之后取而代之成为文明中心，在文化上对克里特文明多有借鉴，但由于本身是野蛮的北方游牧民族，所以在风格方面与克里特文明迥然而异。以巨大建筑著称：希腊神话中的独目巨人三兄弟，由盖亚所生，库克罗比。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>人种特点：据后世考古，克里特人更偏向为埃及人，而&lt;strong>阿卡亚人则是最早的希腊人&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>荷马史诗《&lt;strong>伊利亚特iliad&lt;/strong>》中将希腊人称为阿卡亚人。比如史诗的开篇：歌唱吧，女神，歌唱珀琉斯（Peleus）之子阿喀琉斯（Achilles）的愤怒，这愤怒给阿卡亚人带来了无限的苦难。&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>文明成就：&lt;/p>
&lt;ul>
&lt;li>狮子门、厚几米到十几米的库克罗比城墙；&lt;/li>
&lt;li>以宙斯为首的奥林匹斯神话：父系社会的男神崇拜取代了米诺斯的女神崇拜，喜好穷兵黩武、吃喝玩乐，不再从事生产，代表征服者的形象；&lt;/li>
&lt;li>由线性文字A发展而来线性文字B。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>文明毁灭：公元前1200，被来自北方更加野蛮的民族多利亚人(Dorian)毁灭，废弃线性文字B，从此不被人记起。从此以后，文字毁灭，希腊半岛陷入了长达三百多年的“黑暗时代”，文明大倒退，文化水平极低。&lt;/p>
&lt;p>人民由于黑暗的统治开始大迁徙，文化重新洗牌。其中伊特鲁里亚人迁徙到了意大利中部，被认为是罗马文明的开创者。&lt;/p>
&lt;p>“黑暗时代”又被称为“英雄时代”，因为产生了一批荷马这样的游吟诗人，通过游吟传唱的方式，记述了黑暗时代之前的迈锡尼文明，记录了阿卡亚英雄，将迈锡尼文明传给了后来的希腊城邦文明。
&lt;strong>公元前776年，第一届奥林匹亚竞技会开始，从此成为走出黑暗时代的标志。&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="三-爱琴文明发展简表">三、 爱琴文明发展简表&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">时间&lt;/th>
&lt;th style="text-align: left">文明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">BC26-15&lt;/td>
&lt;td style="text-align: left">优雅的克里特人建立的米诺斯文明&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">BC16-12&lt;/td>
&lt;td style="text-align: left">豪迈的阿卡亚人建立的迈锡尼文明&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">BC12-8&lt;/td>
&lt;td style="text-align: left">野蛮的多利亚人入侵导致的黑暗时代&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description><category domain="https://yinyajun.github.io/categories/%E5%8E%86%E5%8F%B2/">历史</category><category domain="https://yinyajun.github.io/tags/%E5%8F%A4%E5%B8%8C%E8%85%8A/">古希腊</category></item><item><title>有趣的TensorSlow(下)</title><link>https://yinyajun.github.io/posts/ai/basic/tensorslow02/</link><guid isPermaLink="true">https://yinyajun.github.io/posts/ai/basic/tensorslow02/</guid><pubDate>Sun, 02 Jun 2019 22:37:36 +0000</pubDate><author>skyblueice234@gmail.com (雅俊)</author><description>&lt;p>上篇了解了如何用TensorSlow构建计算图并完成前向传播，这里将继续了解计算损失和利用反向传播更新模型参数。&lt;/p>
&lt;h2 id="tensorslow简介">TensorSlow简介&lt;/h2>
&lt;p>TensorSlow是极简的模仿TensorFlow的API的python包。&lt;/p>
&lt;blockquote>
&lt;p>本文是对原作者danielsabinasz的&lt;a href="http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/">教程&lt;/a>的基础上，添加了一点自己的理解。&lt;/p>
&lt;p>TensorSlow &lt;a href="https://github.com/danielsabinasz/TensorSlow">&lt;strong>Github repo地址&lt;/strong>&lt;/a>
TensorSlow原作者&lt;a href="http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/">&lt;strong>英文教程&lt;/strong>&lt;/a>&lt;/p>
&lt;p>&lt;em>The source code has been built with maximal understandability in mind, rather than maximal efficiency.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="perceptron-example">Perceptron Example&lt;/h2>
&lt;p>本文中将会使用TensorSlow处理稍微复杂一点的模型。首先利用上篇的代码，搭建一个表征Perceptron的计算图。后面将以这个Perceptron为例，介绍TensorSlow如何进行损失计算和反向传播的。值得一提的是，这里的输入和参数已经是向量。&lt;/p>
&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d321648b86553eea745db.png" alt="Perceptron计算图">&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">Graph&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">as_default&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create a weight matrix for 2 output classes:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># One with a weight vector (1, 1) for blue and &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># one with a weight vector (-1, -1) for red&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">([&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">session&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">output_probabilities&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">concatenate&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">red_points&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">})&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="反向传播">反向传播&lt;/h2>
&lt;p>上面构建的计算图，能够实现了Perceptron模型的前向传播。目前为止，模型的参数都是初值，已经给定的。这些仅仅为初值的参数效果好吗？那么需要用&lt;strong>损失函数&lt;/strong>来评价当前参数。&lt;/p>
&lt;h3 id="损失函数">损失函数&lt;/h3>
&lt;p>对于二分类问题而言，最大似然估计(MLE)通常是比较好的参数估计的选择。MLE的损失函数形式是负对数似然，也就是交叉熵:&lt;/p>
&lt;p>$$J = - \sum_{i=1}^N \sum_{j=1}^C c_{i, j} \cdot log p_{i, j}$$&lt;/p>
&lt;p>其中，$p_{ij}$代表模型对第$i$个样本预测是第$j$个类别的分数，$c_{ij}$代表第$i$个样本是否是第$j$个类别。同样，损失函数$J$也对应于计算图中的一个operation节点：&lt;/p>
&lt;!--
![添加损失op](https://raw.githubusercontent.com/yinyajun/yinyajun.github.io/master/images/figure/loss_graph.png)
-->
&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d321648b86553eea745d7.png" alt="添加损失op">&lt;/p>
&lt;p>怎么建立这个operation节点？在原计算图的基础上，可以将节点$J$写成这样：
$$\sum_{i=1}^N \sum_{j=1}^C (c \odot log , p)_{i, j}$$&lt;/p>
&lt;p>其中，$c=[c_{i1},\dots,c_{iC}]$是第$i$个样本的label的one-hot向量。可以发现，这是由多个基本的operation节点组成的。实现下面这些基础的operation节点并加以组合，即可得到节点$J$。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">节点&lt;/th>
&lt;th style="text-align: left">描述&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">$log$ 节点&lt;/td>
&lt;td style="text-align: left">The element-wise logarithm&lt;br> of a matrix or vector&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">$\odot$ 节点&lt;/td>
&lt;td style="text-align: left">The element-wise product&lt;br> of two matrices&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">$\sum_{j=1}^C$ 节点&lt;/td>
&lt;td style="text-align: left">Sum over the&lt;br>columns of a matrix&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">$\sum_{i=1}^N$ 节点&lt;/td>
&lt;td style="text-align: left">Sum over the&lt;br>rows of a matrix&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">$-$ 节点&lt;/td>
&lt;td style="text-align: left">Taking the negative&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>基础的operation节点写法在上篇已有介绍，这里仅以log节点为例，代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Operation&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Computes the natural logarithm of x element-wise.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">compute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x_value&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_value&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>最后，组合这些operation节点，可以完整的给出节点$J$并计算出loss：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create a new graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Graph&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">as_default&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">([&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Cross-entropy loss&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">J&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">negative&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">reduce_sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">reduce_sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">multiply&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">session&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">J&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">concatenate&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">red_points&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">+&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">red_points&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="梯度下降">梯度下降&lt;/h3>
&lt;p>上面的步骤已经计算出模型损失函数。损失函数越小，意味着该模型参数下，模型输出和真实标签越接近。而搜索模型参数，使损失函数最小的方法叫做&lt;strong>梯度下降&lt;/strong>，流程如下&lt;/p>
&lt;blockquote>
&lt;ol>
&lt;li>参数$W$和$b$设置随机初始值。&lt;/li>
&lt;li>计算$J$对$W$和$b$的梯度。&lt;/li>
&lt;li>分别在其负梯度的方向上下降一小步(由&lt;code>learning_rate&lt;/code>控制步长)。&lt;/li>
&lt;li>回到step 2，继续执行，直到收敛。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>而&lt;code>GradientDescentOptimizer&lt;/code>就实现了上述流程中的step 3：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">GradientDescentOptimizer&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">learning_rate&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">learning_rate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">learning_rate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">minimize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">learning_rate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">learning_rate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">class&lt;/span> &lt;span class="nc">MinimizationOperation&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Operation&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">compute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad_table&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">compute_gradients&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Iterate all variables&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">type&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Take a step along the direction of the negative gradient&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">value&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="n">learning_rate&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">grad&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">MinimizationOperation&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>上述代码中&lt;code>compute_gradients&lt;/code>函数对应于step 2，将在下一个小节介绍。&lt;code>grad_table&lt;/code>这个字典存放了损失函数$J$节点对图中所有Variable节点的当前梯度（因为只有Variable节点才需要更新）。&lt;/p>
&lt;h3 id="梯度计算">梯度计算&lt;/h3>
&lt;p>梯度的计算根据导数的&lt;strong>链式法则&lt;/strong>。&lt;/p>
&lt;h4 id="链式法则">链式法则&lt;/h4>
&lt;p>链式法则是微积分中的求导法则，用于求一个复合函数的导数。示例如下：&lt;/p>
&lt;!--
![链式法则](https://raw.githubusercontent.com/yinyajun/yinyajun.github.io/master/images/figure/abcde2.png)
-->
&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d311f48b86553eea6f991.png" alt="链式法则">
$$
\begin{aligned}
\frac{\partial e}{\partial a}
&amp;amp;= \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial a}\\
&amp;amp;= \frac{\partial e}{\partial d} \cdot \left( \frac{\partial d}{\partial b} \cdot \frac{\partial b}{\partial a} + \frac{\partial d}{\partial c} \cdot \frac{\partial c}{\partial a} \right)\\
&amp;amp;= \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial b} \cdot \frac{\partial b}{\partial a} + \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial c} \cdot \frac{\partial c}{\partial a}
\end{aligned}
$$&lt;/p>
&lt;h3 id="梯度计算-1">梯度计算&lt;/h3>
&lt;p>通过上面的链式法则，计算损失函数节点（记为loss节点）对当前节点$n$的梯度，也是链式的：&lt;/p>
&lt;ol>
&lt;li>计算loss节点对当前$n$节点的consumer节点的输出的梯度$G$.&lt;/li>
&lt;li>计算$n$节点的consumer节点对$n$节点梯度$\times G$，将所有consumer节点计算出的梯度全部相加。&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code># 计算当前节点梯度的伪代码
grad_n = grad_child1 * grad_child1_n + ...
+ grad_childk * grad_childk_n
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>&lt;code>grad_n&lt;/code> : $\partial\text{loss}/\partial{n}$&lt;/li>
&lt;li>&lt;code>chlid1&lt;/code>: a child node of node $n$&lt;/li>
&lt;li>&lt;code>grad_child1&lt;/code>: $\partial\text{loss}/\partial{\text{child1}}$&lt;/li>
&lt;li>&lt;code>grad_child1_n&lt;/code>: $\partial\text{child1}/\partial{n}$&lt;/li>
&lt;/ul>
&lt;p>先计算子节点梯度，然后得到当前节点梯度，完全按照链式法则。由于当前节点的梯度计算依赖子节点的梯度计算，因此可以使用拓扑排序。而作者使用了BFS来完成拓扑排序。从反图角度而言，每个节点的入度都为0，可以放心使用BFS来完成拓扑排序。&lt;/p>
&lt;p>通过一步步迭代，能够得到所有节点的梯度。其中$\partial\text{child1}/\partial{n}$，也就是子节点的输出对于该节点的输出的梯度如何计算？这个工作应该由子节点来完成。&lt;/p>
&lt;p>对于一个operation节点，提前定义这个operation的梯度计算函数，并使用装饰器&lt;code>@RegisterGradient&lt;/code>来将实现了计算梯度函数的opeartion节点注册到全局变量&lt;code>_gradient_registry&lt;/code>中，具体细节见下一小节。整体的梯度计算过程如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">queue&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Queue&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_gradients&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad_table&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Perform a breadth-first search, backwards from the loss&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">set&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Queue&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">put&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">queue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">queue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Iterate all consumers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">consumer&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">consumers&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lossgrad_wrt_consumer_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">consumer&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Retrieve the function which computes gradients with respect to&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># consumer&amp;#39;s inputs given gradients with respect to consumer&amp;#39;s output.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">consumer_op_type&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">consumer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="vm">__class__&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bprop&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">_gradient_registry&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">consumer_op_type&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Get the gradient of the loss with respect to all of consumer&amp;#39;s inputs&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lossgrads_wrt_consumer_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">bprop&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">consumer&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lossgrad_wrt_consumer_output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">consumer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_nodes&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># If there is a single input node to the consumer, lossgrads_wrt_consumer_inputs is a scalar&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">lossgrads_wrt_consumer_inputs&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Otherwise, lossgrads_wrt_consumer_inputs is an array of gradients for each input node&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node_index_in_consumer_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">consumer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_nodes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Get the gradient of the loss with respect to node&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lossgrad_wrt_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">lossgrads_wrt_consumer_inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node_index_in_consumer_inputs&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">lossgrad_wrt_node&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">hasattr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;input_nodes&amp;#34;&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">input_node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_nodes&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">input_node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">put&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">grad_table&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中一些细节还需要自己体会代码。&lt;/p>
&lt;h3 id="operation节点的梯度">Operation节点的梯度&lt;/h3>
&lt;p>根据operation操作的&lt;code>compute&lt;/code>方法给出节点正向传播的函数，据此计算梯度函数，然后注册到全局变量中即可。&lt;/p>
&lt;p>以矩阵乘法&lt;code>matmul&lt;/code>为例：给定对于$AB$的梯度$G$，其对于$A$的梯度是$GB^T$，对于$B$的梯度是$A^TG$，所以该节点的梯度是一个向量。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="nd">@RegisterGradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;matmul&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">_matmul_gradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">op&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">grad&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">A&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">op&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">B&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">op&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">grad&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grad&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>同样的&lt;code>sigmoid&lt;/code>的梯度可以写为$G \cdot \sigma(a) \cdot \sigma(1-a)$&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="nd">@RegisterGradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;sigmoid&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">_sigmoid_gradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">op&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">grad&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sigmoid&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">op&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">grad&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sigmoid&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">sigmoid&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="perceptron-train">Perceptron Train&lt;/h2>
&lt;p>完整的Perceptron模型训练代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create a new graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Graph&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">as_default&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Initialize weights randomly: step 1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">J&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">negative&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">reduce_sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">reduce_sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">multiply&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># step 2 and step 3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">minimization_op&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">GradientDescentOptimizer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">learning_rate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.01&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">minimize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">J&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">feed_dict&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">concatenate&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">red_points&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">+&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">red_points&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">session&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Perform 100 gradient descent steps, step 4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">step&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">100&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">J_value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">J&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">step&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Step:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">step&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34; Loss:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">J_value&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">minimization_op&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">W&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Weight matrix:&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W_value&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b_value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Bias:&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b_value&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>Step: 0 Loss: 202.782788396
Step: 10 Loss: 4.04566479054
Step: 20 Loss: 2.69644468305
Step: 30 Loss: 2.00506261735
Step: 40 Loss: 1.62202006027
Step: 50 Loss: 1.39268559111
Step: 60 Loss: 1.24498439759
Step: 70 Loss: 1.14348265257
Step: 80 Loss: 1.06965484385
Step: 90 Loss: 1.01324253829
Weight matrix:
[[ 1.27496197 -1.77251219]
[ 1.11820232 -2.01586474]]
Bias:
[-0.45274057 -0.39071841]
&lt;/code>&lt;/pre>
&lt;p>可以发现loss的确是不断降低的，模型参数不断优化更新。&lt;/p>
&lt;h2 id="multi-layer-perceptrons">Multi-Layer Perceptrons&lt;/h2>
&lt;p>使用TensorSlow来处理更加复杂的问题：分类的决策边界更加复杂。这里也搭建了更加复杂的模型MLP。&lt;/p>
&lt;h3 id="数据分布">数据分布&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">matplotlib.pyplot&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">plt&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">tensorslow&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">ts&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create two clusters of red points centered at (0, 0) and (1, 1), respectively.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">red_points&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">concatenate&lt;/span>&lt;span class="p">((&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mf">0.2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">25&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">25&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mf">0.2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">25&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">25&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create two clusters of blue points centered at (0, 1) and (1, 0), respectively.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">blue_points&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">concatenate&lt;/span>&lt;span class="p">((&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mf">0.2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">25&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">25&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mf">0.2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">25&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">25&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Plot them&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">red_points&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">red_points&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">color&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;red&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">blue_points&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">color&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;blue&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d393a48b86553eea9d311.png" alt="真实数据分布">&lt;/p>
&lt;h3 id="计算图">计算图&lt;/h3>
&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d393a48b86553eea9d313.png" alt="MLP计算图">&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create a new graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Graph&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">as_default&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create training input placeholder&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create placeholder for the training classes&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build a hidden layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_hidden1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b_hidden1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p_hidden1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sigmoid&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W_hidden1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b_hidden1&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build a hidden layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_hidden2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">8&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b_hidden2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">8&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p_hidden2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sigmoid&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p_hidden1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W_hidden2&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b_hidden2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build a hidden layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_hidden3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">8&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b_hidden3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p_hidden3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sigmoid&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p_hidden2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W_hidden3&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b_hidden3&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build the output layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p_hidden3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W_output&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b_output&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build cross-entropy loss&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">J&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">negative&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reduce_sum&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reduce_sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">multiply&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p_output&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build minimization op&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">minimization_op&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">train&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">GradientDescentOptimizer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">learning_rate&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.03&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">minimize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">J&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build placeholder inputs&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">feed_dict&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">concatenate&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">red_points&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">+&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">red_points&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create session&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">session&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Perform 100 gradient descent steps&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">step&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2000&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">J_value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">J&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">step&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">100&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Step:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">step&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34; Loss:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">J_value&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">minimization_op&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>Step: 0 Loss: 105.25316761015766
Step: 100 Loss: 54.82276887616324
Step: 200 Loss: 18.531741905961816
Step: 300 Loss: 10.88319073941583
Step: 400 Loss: 5.167908735173651
Step: 500 Loss: 4.015258056948531
...
Step: 1400 Loss: 0.1973992659737359
Step: 1500 Loss: 0.17700496511514013
Step: 1600 Loss: 0.1602628699213534
Step: 1700 Loss: 0.14629349101006833
Step: 1800 Loss: 0.13447502395360536
Step: 1900 Loss: 0.1243562427509077
&lt;/code>&lt;/pre>
&lt;h3 id="可视化">可视化&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Visualize classification boundary&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">xs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ys&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">pred_classes&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">xs&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">ys&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pred_class&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p_output&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">feed_dict&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">]]})[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pred_classes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pred_class&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">()))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">xs_p&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ys_p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[],&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">xs_n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ys_n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[],&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">pred_classes&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xs_n&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ys_n&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xs_p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ys_p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xs_p&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ys_p&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;ro&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">xs_n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ys_n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;bo&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d393a48b86553eea9d30f.png" alt="决策边界">&lt;/p>
&lt;h2 id="小结">小结&lt;/h2>
&lt;p>短小精悍的TensorSlow揭示了了深度模型框架的基本工作机理。当然，它只适用于教学，生产环境下的深度学习框架软件将更加复杂。&lt;/p></description><category domain="https://yinyajun.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</category><category domain="https://yinyajun.github.io/tags/tensorslow/">TensorSlow</category></item><item><title>有趣的TensorSlow(上)</title><link>https://yinyajun.github.io/posts/ai/basic/tensorslow01/</link><guid isPermaLink="true">https://yinyajun.github.io/posts/ai/basic/tensorslow01/</guid><pubDate>Sat, 01 Jun 2019 22:37:36 +0000</pubDate><author>skyblueice234@gmail.com (雅俊)</author><description>&lt;p>TensorFlow的许多概念，如graph, session, operation等，为什么要这么设计？Github上的TensorSlow用纯python来模仿了TF的底层api，加深理解TF中的底层概念。&lt;/p>
&lt;h2 id="tensorslow简介">TensorSlow简介&lt;/h2>
&lt;ul>
&lt;li>极简的模仿TensorFlow的API的python包。&lt;/li>
&lt;li>使用纯python作为后端。&lt;/li>
&lt;li>仅用作教学目的，帮助理解TensorFlow底层原理。&lt;/li>
&lt;li>代码量非常少，跟着教程走，很容易看完。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>本文是对原作者danielsabinasz的&lt;a href="http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/">教程&lt;/a>
的基础上，添加了一点自己的理解。&lt;/p>
&lt;p>TensorSlow &lt;a href="https://github.com/danielsabinasz/TensorSlow">&lt;strong>Github repo地址&lt;/strong>&lt;/a>
TensorSlow原作者&lt;a href="http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/">&lt;strong>英文教程&lt;/strong>&lt;/a>&lt;/p>
&lt;p>&lt;em>The source code has been built with maximal understandability in mind, rather than maximal efficiency.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="计算图computational-graphs">计算图Computational Graphs&lt;/h2>
&lt;p>计算图是一种有向图，它是以图的形式来表示或计算数学函数。和普通的图一样，计算图中也有节点和边。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>节点&lt;/strong>：要么是提供输入数据的节点，要么是代表操作数据的函数的节点。&lt;/li>
&lt;li>&lt;strong>边&lt;/strong>：函数参数（或者说数据依赖），以流的形式为节点传输数据。&lt;/li>
&lt;li>&lt;strong>Tensor&lt;/strong>: 节点的输入和输出数据，其实就是一个多维的array。&lt;/li>
&lt;/ul>
&lt;p>下图展示了一个计算图，它描述了怎么计算输入节点$x$和$y$的和$z$的过程。&lt;/p>
&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d3a8248b86553eeaa37eb.png" alt="计算图">&lt;/p>
&lt;p>$x$和$y$都是$z$的输入节点，而$z$是$x$和$y$的消费节点。节点$z$描述了这么一个方程：&lt;/p>
&lt;p>$$z:\mathcal{R}^2 \rightarrow \mathcal{R}, z(x,y) = x + y.$$&lt;/p>
&lt;p>计算图这个概念非常有用，特别当计算非常复杂的时候，下面的例子对应于一个仿射变换:
$$z(A,x,b)= Ax+b$$&lt;/p>
&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d3a8248b86553eeaa37ed.png" alt="仿射变换的计算图">&lt;/p>
&lt;p>首先了解各类型节点的表示，从节点输入，节点输出和节点操作来考察各类型节点。&lt;/p>
&lt;h3 id="operations节点">Operations节点&lt;/h3>
&lt;p>每个operation节点以下面三个表征：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>节点操作&lt;/strong>：当operation节点的输入的值给定时，用&lt;code>compute&lt;/code>函数来计算该operation节点的输出&lt;/li>
&lt;li>&lt;strong>节点输入&lt;/strong>：&lt;code>input_nodes&lt;/code>列表，可以是varibles节点或者是其他operations节点&lt;/li>
&lt;li>&lt;strong>节点输出&lt;/strong>：&lt;code>consumers&lt;/code>列表，它们将该operation节点的输出作为它们的输入。&lt;/li>
&lt;/ul>
&lt;p>上面三个含义都是显而易见的描述operation节点的操作，在&lt;code>Opearation&lt;/code>类中，用三个成员表示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Operation&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">input_nodes&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[]):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_nodes&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">input_nodes&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Initialize list of consumers &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">consumers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Append this operation to the list of consumers of all input nodes&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">input_node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">input_nodes&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">consumers&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Append this operation to the list of operations in the currently active default graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_default_graph&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">operations&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">compute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">pass&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>compute&lt;/code>方法是需要每个operation节点子类去实现。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Addition Operation节点示例&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Operation&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Returns x + y element-wise.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">compute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x_value&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_value&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">x_value&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_value&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">x_value&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">y_value&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Matrix Multiplicaiton Operation节点示例&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Operation&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Multiplies matrix a by matrix b, producing a * b.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">compute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">a_value&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b_value&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">a_value&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b_value&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">a_value&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">b_value&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="placeholder节点">Placeholder节点&lt;/h3>
&lt;p>计算图中为了计算输出，必须要向图中提供一次输入数据。而Placeholder节点，正如其名，就是用来干这事的。在仿射变换计算图的例子中，$x$就是这种节点。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>节点操作&lt;/strong>：无&lt;/li>
&lt;li>&lt;strong>节点输入&lt;/strong>：无&lt;/li>
&lt;li>&lt;strong>节点输出&lt;/strong>：&lt;code>consumers&lt;/code>列表，它们将该节点的输出作为它们的输入。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">placeholder&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Represents a placeholder node that has to be provided with a value
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> when computing the output of a computational graph
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">consumers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_default_graph&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">placeholders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="variables节点">Variables节点&lt;/h3>
&lt;p>在仿射变换的例子中，$x$,$A$和$b$都不是operation节点，但是$x$与$A$和$b$有一些区别，$x$是纯粹的输入placeholder节点，而$A$和$b$是能不断变更输出值，它们是Variable节点。这些Variable节点虽然没有输入，但本身有初值。Variable节点是计算图的固有成分。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>节点操作&lt;/strong>：无&lt;/li>
&lt;li>&lt;strong>节点输入&lt;/strong>：无&lt;/li>
&lt;li>&lt;strong>节点输出&lt;/strong>：&lt;code>consumers&lt;/code>列表，它们将该节点的输出作为它们的输入。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Variable&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Represents a variable (i.e. an intrinsic, changeable parameter of a computational graph).
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">initial_value&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">initial_value&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">consumers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_default_graph&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">variables&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="the-graph-class">The Graph class&lt;/h3>
&lt;p>使用&lt;code>Graph&lt;/code>来绑定所有创建的节点。当创建新的graph的时候，可以调用&lt;code>as_default&lt;/code>方法来设置默认图&lt;code>_default_graph&lt;/code>
，这样我们不用显示地去将节点绑定到图中。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Graph&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Construct Graph&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">operations&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">placeholders&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">variables&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">as_default&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">global&lt;/span> &lt;span class="n">_default_graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_default_graph&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="example">Example&lt;/h3>
&lt;p>通过已经建立的类，来建立仿射变换例子的计算图：&lt;/p>
&lt;p>$$
z = \begin{pmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; -1
\end{pmatrix}
\cdot
x&lt;/p>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>\begin{pmatrix}
1 \\
1
\end{pmatrix}
$$&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create a new graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Graph&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">as_default&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create variables&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create placeholder&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create hidden node y&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create output node z&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">z&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="session">Session&lt;/h3>
&lt;p>建立完计算图，那么开始思考，如何计算出节点的输出？输出节点通常是operation节点。为了正确计算输出节点的输出，需要按正确的顺序计算。仍以仿射变换为例，要计算$z$必须先计算出中间结果$y$。也就是说，
&lt;strong>必须保证节点是按顺序执行的，计算节点$o$之前，节点$o$的所有输入节点已经完成计算&lt;/strong>，使用&lt;strong>拓扑排序&lt;/strong>即可满足要求。&lt;/p>
&lt;p>拓扑排序，是原图的reverse post order，和反图的post order一致。这里使用反图的post order来得到拓扑顺序。这一系列计算都封装在了Session中。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Session&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Represents a particular execution of a computational graph.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">operation&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{}):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nodes_postorder&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">traverse_postorder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">operation&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Iterate all nodes to determine their value&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">nodes_postorder&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">type&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Set the node value to the placeholder value from feed_dict&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">elif&lt;/span> &lt;span class="nb">type&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Set the node value to the variable&amp;#39;s value attribute&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">value&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="c1"># Operation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Get the input values for this operation from node_values&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">input_node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">input_node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_nodes&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Compute the output of this operation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">compute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Convert lists to numpy arrays&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">type&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Return the requested node value&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">operation&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">traverse_postorder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">operation&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nodes_postorder&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">recurse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">isinstance&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Operation&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">input_node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_nodes&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">recurse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nodes_postorder&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">recurse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">operation&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">nodes_postorder&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可见，&lt;code>run&lt;/code>方法对要计算的operation节点进行了一次拓扑排序，按照这个顺序，依次计算节点。&lt;/p>
&lt;h4 id="example-1">Example&lt;/h4>
&lt;p>完成仿射变换例子的输出.&lt;/p>
&lt;p>$$
z=\begin{pmatrix}1 &amp;amp; 0 \\ 0 &amp;amp; -1 \end{pmatrix} \cdot \begin{pmatrix}1 \\ 2\end{pmatrix} + \begin{pmatrix}
1 \\
1
\end{pmatrix}=
\begin{pmatrix}
2 \\
-1
\end{pmatrix}
$$&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">session&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">})&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>[ 2 -1]
&lt;/code>&lt;/pre>
&lt;h2 id="小结">小结&lt;/h2>
&lt;p>目前，已经可以搭建计算图，用来计算一些复杂函数了。如果用计算图来搭建神经网络，目前的代码完全能够完成网络的前向传播。&lt;a href="https://yinyajun.github.io/infomation-tech/tensorslow-02/">下篇&lt;/a>
将会涉及到loss计算及反向传播在计算图中如何实现。&lt;/p></description><category domain="https://yinyajun.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</category><category domain="https://yinyajun.github.io/tags/tensorslow/">TensorSlow</category></item></channel></rss>