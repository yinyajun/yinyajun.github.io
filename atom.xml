<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh-CN"><title type="text">BitTrace</title><subtitle type="html">yajun的学习笔记</subtitle><updated>2025-01-02T04:11:40+00:00</updated><id>https://yinyajun.github.io/</id><link rel="alternate" type="text/html" href="https://yinyajun.github.io/"/><link rel="self" type="application/atom+xml" href="https://yinyajun.github.io/atom.xml"/><author><name>雅俊</name><uri>https://yinyajun.github.io/</uri><email>skyblueice234@gmail.com</email></author><generator uri="https://gohugo.io/" version="0.135.0">Hugo</generator><entry><title type="text">赵世钰《强化学习》复习笔记01</title><link rel="alternate" type="text/html" href="https://yinyajun.github.io/posts/ai/rl/math_rl_note01/"/><id>https://yinyajun.github.io/posts/ai/rl/math_rl_note01/</id><updated>2025-01-02T12:11:21+08:00</updated><published>2023-07-15T15:37:36+00:00</published><author><name>雅俊</name><uri>https://yinyajun.github.io/</uri><email>skyblueice234@gmail.com</email></author><summary type="html">我的强化学习入门课程就是赵世钰老师的&lt;a href="https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning">强化学习&lt;/a>，二刷后记录一些复习笔记。这篇是1-3章的复习笔记。</summary><content type="html">&lt;p>我的强化学习入门课程就是赵世钰老师的&lt;a href="https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning">强化学习&lt;/a>，二刷后记录一些复习笔记。这篇是1-3章的复习笔记。&lt;/p>
&lt;h2 id="第一章">第一章&lt;/h2>
&lt;h3 id="基础概念">基础概念&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">概念&lt;/th>
&lt;th style="text-align: left">解释&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">State &amp;amp; Action&lt;/td>
&lt;td style="text-align: left">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">State Transition&lt;/td>
&lt;td style="text-align: left">从一个state跳转到另一个state的条件概率&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Reward&lt;/td>
&lt;td style="text-align: left">state下采取某一action后环境给的奖励&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Policy&lt;/td>
&lt;td style="text-align: left">根据state采取action的条件概率&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Trajectory&lt;/td>
&lt;td style="text-align: left">从某个state开始的state-action-reward chain&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Return&lt;/td>
&lt;td style="text-align: left">Trajectory中的所有reward总和&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Discounted Return&lt;/td>
&lt;td style="text-align: left">对于无限长的Trajectory，可以得到收敛的Return&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Episode/Trial&lt;/td>
&lt;td style="text-align: left">agent会停在terminal state的trajectory&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>有了这些基础概念，那么可以来构建马尔可夫决策过程。&lt;/p>
&lt;h3 id="马尔可夫决策过程mdp">马尔可夫决策过程（MDP）&lt;/h3>
&lt;p>MDP是在马尔可夫过程（MP）的基础上，MP有状态转移+马尔可夫性质。&lt;/p>
&lt;ul>
&lt;li>在状态跳转后加上reward，就到了马尔可夫奖励过程。&lt;/li>
&lt;li>状态跳转前在加上action，就到了MDP。&lt;/li>
&lt;/ul>
&lt;p>MDP就是强化学习的数学框架。那么MDP有哪些核心要素？&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>State space&lt;/strong>：$\mathcal{S}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Action space&lt;/strong>: $\mathcal{A(s)}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Reward set&lt;/strong>: ${\mathcal{R(s,a)}}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>State transition prob&lt;/strong>: $p(s'|s,a)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Reward prob&lt;/strong>: $p(r|s,a)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Policy&lt;/strong>: $\pi(a|s)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Markov Property&lt;/strong>（MDP version）&lt;/p>
&lt;p>next state or reward &lt;strong>only&lt;/strong> depends on the current state and action.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>有了这些核心要素，那么MDP形式上就是&lt;/p>
&lt;p>$$S_1 \rightarrow A_1 \rightarrow R_2 \rightarrow S_2 \rightarrow A_2 \rightarrow R_3 \dots$$&lt;/p>
&lt;p>需要找到一个Policy，来最大化这个Return。&lt;/p>
&lt;blockquote>
&lt;p>如果采取固定的policy，那么这里的状态转移就退化为MP的状态转移，MDP也退化为MP。&lt;/p>
&lt;/blockquote>
&lt;h3 id="环境模型">环境模型&lt;/h3>
&lt;p>在MDP中，$p(s'|s,a)$和$p(r|s,a)$是由环境决定的model（dynamics）。&lt;/p>
&lt;p>它们不是我们求解的目标，但是也会影响整个MDP。&lt;/p>
&lt;ul>
&lt;li>如果环境model已知（白盒环境），求解方法为Model-based.&lt;/li>
&lt;li>如果环境model未知（黑盒环境），求解方法为Model-free.&lt;/li>
&lt;/ul>
&lt;h2 id="第二章">第二章&lt;/h2>
&lt;p>这章只有一个东西，就是state value。然后围绕state value的求解，引出了Bellman方程。&lt;/p>
&lt;blockquote>
&lt;p>首先，好的Policy是怎么eval的？用Return，特别是discounted return。&lt;/p>
&lt;/blockquote>
&lt;p>怎么计算return？&lt;/p>
&lt;ol>
&lt;li>Definition: $$v_1 = r_1 + \gamma r_2 + \gamma^2 r_3 + ...$$&lt;/li>
&lt;li>Bootstrapping: $$v_1=r_1 + \gamma(r_2 + \gamma r_3 + ...) = r_1 + \gamma v_2$$&lt;/li>
&lt;/ol>
&lt;p>bootstrapping的方式乍看很绕，其实就是递归的定义。具体而言，从一个状态出发得到的return，依赖其他状态出发得到的return。&lt;/p>
&lt;p>对于所有的状态，都有这么一个方程，联立所有方程，可以得到矩阵形式&lt;/p>
&lt;p>$$v = r + \gamma P v$$&lt;/p>
&lt;p>其实这就是Bellman方程。现在这个形式不太方便使用。&lt;/p>
&lt;h3 id="state-values">State Values&lt;/h3>
&lt;p>更正式的提出了trajectory的discounted return&lt;/p>
&lt;p>$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...$$&lt;/p>
&lt;p>这是个随机变量，因为所有的R都是随机变量。为了避免return的随机性对估计policy的影响，引入了state value。&lt;/p>
&lt;p>$$v_\pi(s) = \mathbb{E}[G_t| S_t=s]$$&lt;/p>
&lt;p>它的定义很直接，就是return的期望。至此，我们将使用state value代替return来eval policy。&lt;/p>
&lt;h3 id="bellman方程">Bellman方程&lt;/h3>
&lt;p>同样引入递归定义&lt;/p>
&lt;p>$$
\begin{aligned}
G_t &amp;amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...\
&amp;amp; = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + ...)\
&amp;amp; = R_{t+1} + \gamma G_{t+1}
\end{aligned}
$$&lt;/p>
&lt;p>$$
\begin{aligned}
\frac{\partial e}{\partial a}
&amp;amp;= \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial a}\\
&amp;amp;= \frac{\partial e}{\partial d} \cdot \left( \frac{\partial d}{\partial b} \cdot \frac{\partial b}{\partial a} + \frac{\partial d}{\partial c} \cdot \frac{\partial c}{\partial a} \right)\\
&amp;amp;= \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial b} \cdot \frac{\partial b}{\partial a} + \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial c} \cdot \frac{\partial c}{\partial a}
\end{aligned}
$$&lt;/p>
&lt;p>含义很自然，这一时刻的return = 当前reward + 下一时刻的return。&lt;/p>
&lt;p>因此，state value的可以拆为
$$v_\pi(s) = \mathbb{E} [R_{t+1} | S_t = s] + \gamma \mathbb{E} [ G_{t+1} | S_t = s]$$&lt;/p>
&lt;ul>
&lt;li>
&lt;p>前者是immediate reward。&lt;/p>
&lt;p>是当前状态所能获取的reward的期望。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>后者是future rewards。&lt;/p>
&lt;p>当前状态开始，跳到下一时刻的某个状态后的return的期望。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="immediate-reward">immediate reward&lt;/h4>
&lt;p>$$\mathbb{E}[R_{t+1}| S_t=s] = \Sigma_{a} \pi(a|s) \Sigma _r p(r|s,a)r$$&lt;/p>
&lt;p>随机性在于policy和环境奖励的随机。&lt;/p>
&lt;h4 id="future-rewards">future rewards&lt;/h4>
&lt;p>$$
\begin{aligned}
&amp;amp;\mathbb{E}[G_{t+1}| S_t = s] \
&amp;amp;= \Sigma_{s'} \mathbb{E}[G_{t+1}| S_{t+1}=s'] p(s'|s) \
&amp;amp;= \Sigma_{s'}v_\pi (s') p(s'|s)
\end{aligned}
$$&lt;/p>
&lt;p>这里比较跳的步骤是用了markov的性质，Return只和当前时刻的State有关，和前一个时刻的State无关。&lt;/p>
&lt;p>随机性在于return的随机和状态转移的随机。&lt;/p>
&lt;h4 id="小说明">小说明&lt;/h4>
&lt;p>这章在引入了两次递归定义，一次是在Return的bootstrapping求解，一次在State value的定义。&lt;/p>
&lt;p>初学的时候，觉得有点混淆，因为这两个好像是同一个东西。它们的区别在于&lt;/p>
&lt;ul>
&lt;li>前者是实现，用小写字母表示。某个state的return依赖其他state的return。&lt;/li>
&lt;li>后者是随机变量，用大写字母表示。某个时刻state的return依赖未来时刻的return。&lt;/li>
&lt;li>因为Return是随机变量，当具体采样到某条trajectory时，才是前者。&lt;/li>
&lt;li>而我们为了更好的衡量Return，采用期望而不是某一个采样。&lt;/li>
&lt;/ul>
&lt;h4 id="bellman-equation">Bellman Equation&lt;/h4>
&lt;p>综上，&lt;/p>
&lt;p>$$v_\pi(s) = \mathbb{E} [R_{t+1} | S_t = s] + \gamma \mathbb{E} [ G_{t+1} | S_t = s]$$&lt;/p></content><category scheme="https://yinyajun.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" term="强化学习" label="强化学习"/><category scheme="https://yinyajun.github.io/tags/%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6/" term="西湖大学" label="西湖大学"/><category scheme="https://yinyajun.github.io/tags/%E8%B5%B5%E4%B8%96%E9%92%B0/" term="赵世钰" label="赵世钰"/></entry><entry><title type="text">源远流长的希腊神话传说</title><link rel="alternate" type="text/html" href="https://yinyajun.github.io/posts/history/greek02/"/><id>https://yinyajun.github.io/posts/history/greek02/</id><updated>2025-01-01T21:07:29+08:00</updated><published>2019-07-04T21:57:10+00:00</published><author><name>雅俊</name><uri>https://yinyajun.github.io/</uri><email>skyblueice234@gmail.com</email></author><summary type="html">Mooc上武汉大学哲赵林老师主讲的《西方文化概论》之古希腊文化部分的13讲课程。本篇对应于3-5讲。</summary><content type="html">&lt;h2 id="希腊神话的源流">希腊神话的源流&lt;/h2>
&lt;p>在克里特文明时期，已经有了流传甚广的神话传说，有的随着克里特文明的毁灭而消失，有的则在迈锡尼时期与阿卡亚人的神话传说相融合。&lt;/p>
&lt;h3 id="克里特时期">克里特时期&lt;/h3>
&lt;p>带有浓重的埃及成分，埃及的迷宫、半人半兽的神灵都带有&lt;strong>神秘色彩&lt;/strong>，&lt;strong>能工巧匠&lt;/strong>辈出，像木乃伊，以现代的科技手段依然难以复制，克里特也带有这样的色彩（精巧和扑朔迷离）。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>米诺陶诺斯神话&lt;/strong>：传说米诺斯国王在王宫中间养了一头怪牛，是他的妻子与牛所生，每9年向雅典索要7对童男童女来喂养，第三次进贡（置于为什么进贡，可以查看忒休斯的生平故事）的时候有一个少年忒休斯来到王宫，与米诺斯的女儿阿里阿德涅相爱。女儿为了帮助他给他一把剑和一段绳索，忒休斯来到迷宫前，将绳索一头系在门口，深入迷宫杀死怪牛又出来。后成为成语“阿里阿德涅之线”（Ariadne's thread，比喻走出迷宫的方法和路径，解决复杂问题的线索。）。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>代达罗斯神话&lt;/strong>：代达罗斯是迷宫的修建者，米诺斯为了长期差遣代达罗斯就将他囚禁在王宫里。代达罗斯为了将儿子伊卡洛斯送出，建造了一对翅膀，用蜡粘在身上。飞出后，很骄傲，欲与天公试比高，飞近了太阳，太阳神阿波罗将蜡融化，翅膀掉落，儿子就掉海里摔死了。
值得一提的是，IceFrog将Dota1中的装备“暴雪弩炮”，在Dota2中改名为“代达罗斯之殇”，而凤凰的名字也叫“伊卡洛斯”，其中深意可以想象。&lt;/p>
&lt;/blockquote>
&lt;h3 id="迈锡尼神话">迈锡尼神话&lt;/h3>
&lt;p>阿卡亚人在入侵伯罗奔尼撒岛之前，曾居住在马其顿，周围最高的山叫奥林匹斯山，因为神的至高无上的地位，他们开始崇拜奥林匹斯山。奥林匹斯神族中的那些作为征服者的神（宙斯、阿瑞斯、阿波罗、阿尔忒弥斯、赫尔墨斯等）和英雄（阿伽门农、赫拉克勒斯等）随着印欧语入侵者阿卡亚人一同进入希腊半岛。&lt;/p>
&lt;p>闪米特语系崇拜女神，而印欧语系崇拜男神（同样出现在其他印欧语系入侵者的神话中，比如北欧神话）。所以阿卡亚人崇拜的神以男性为主（主神宙斯），崇拜英雄，崇拜战争，这和克里特神话中精巧的特性不太一样。&lt;/p>
&lt;p>阿卡亚人崇拜的神是同人同性的，甚至比人更人，为人之典范，其为神并不因智慧，而是粗犷的形体之美。而经过埃及和克里特文明流传的半人半兽形态的神尽管也有保留，但是大都成为了反面形象，例如地狱三头狗、头发都是蛇看谁就成石头的美杜莎、用歌声催眠的塞壬女妖以及狮身人面的斯芬克斯等。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>赫拉克勒斯Hercules&lt;/strong> ：大力士，是主神宙斯与阿尔克墨涅之子。干了12件惊天动地的大事，被多利亚人奉为先祖，曾活剥一头最凶悍的狮子，披在身上。
&lt;strong>雅典娜Athena&lt;/strong>：智慧女神、女战神，全身披挂，威风凛凛。
&lt;strong>维纳斯Venus&lt;/strong>：美神 阿佛洛狄特Aphrodite
&lt;strong>阿喀琉斯Achilles&lt;/strong>：特洛伊战争主角，海洋女神忒提斯（Thetis）和英雄珀琉斯（Peleus）之子
&lt;strong>阿伽门农Agamemnon&lt;/strong>：迈锡尼国王，希腊联合远征军统帅&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Coldplay单曲《&lt;strong>Something Just Like This&lt;/strong>》中的歌词：
Achilles and his gold
Hercules and his gifts&lt;/p>
&lt;/blockquote>
&lt;h3 id="地理梳理">地理梳理&lt;/h3>
&lt;p>希腊本土主要为两个半岛组成，一个是伯罗奔尼撒半岛，以中央的伯罗奔尼撒平原为名，另一个是阿提卡半岛。
前者有许多重要的城邦，比如迈锡尼和后来的斯巴达，还有奥林匹亚，奥林匹亚是每隔四年祭奠奥林匹斯山诸神的地方，即后来的奥林匹亚竞技会，公元前776年为第一届。后者有今天希腊的首都雅典、底比斯、马拉松。再往北，有城邦德尔斐，建有祭奠阿波罗的神庙，因其神谕灵验，在希腊人心中地位神圣。&lt;/p>
&lt;h3 id="希腊神话源流小结">希腊神话源流小结&lt;/h3>
&lt;p>荷马史诗中的特洛伊战争，赫西俄德神谱中奥林匹斯神族和泰坦神族的战斗，这些神话传说反映了了希腊入侵者（阿卡亚人、多利亚人等）对爱琴海世界的血与火的征服活动。美轮美奂的希腊神话传说是历史融合的结果，既包含埃及、巴比伦等神话的神秘色彩，又融入了北方入侵者的粗犷风格。&lt;/p>
&lt;ol>
&lt;li>埃及宗教的影响（精巧、扑朔迷离、半人半兽）&lt;/li>
&lt;li>北方游牧民族神话的影响（战神、征服者，英雄）&lt;/li>
&lt;li>巴比伦神话的影响（巴比伦史诗&lt;em>恩努马-艾利希Enuma Elish&lt;/em>，谱系分明，又称《神谱》）&lt;/li>
&lt;/ol>
&lt;h2 id="荷马史诗和系统叙述诗">荷马史诗和系统叙述诗&lt;/h2>
&lt;p>神话的融合虽然是悄无声息的，或者说通过激烈的战争碰撞。在这种无意识的交融过程中，荷马和赫西俄德以及大量的佚名诗人起到了穿针引线的作用。更重要的是，在黑暗时代，将古老的克里特、迈锡尼神话传说传递给后来的希腊城邦时代。&lt;/p>
&lt;h3 id="荷马背景">荷马背景&lt;/h3>
&lt;p>生活在公元前九世纪末八世纪初，即黑暗时代即将过去，城邦时代即将来临的时期。他双目失明，像他的许多前辈一样游吟传唱，所唱的是个个不同的小故事，但是却将希腊的诸神系统地联系起来，形成波澜壮阔、内容庞杂的百科全书式的史诗。&lt;/p>
&lt;h3 id="史诗背景">史诗背景&lt;/h3>
&lt;p>&lt;strong>以迈锡尼时期的英雄为主角&lt;/strong>，讲述的是神和英雄的故事及他们的血缘关系。英雄是半神，是神和凡人的子嗣（hero，半神）。两部史诗所记载的故事都是对迈锡尼时期曾经发生过的战争和航海活动的一种神话化的渲染，是对阿卡亚人以往的英雄业绩的赞美讴歌。&lt;/p>
&lt;h3 id="伊利亚特">《伊利亚特》&lt;/h3>
&lt;p>主要内容是特洛伊战争，希腊英雄或阿卡亚英雄与特洛伊英雄的战争，因双方与神灵有血缘关系，所以神也加入了战争。&lt;/p>
&lt;ol>
&lt;li>战争背景：伊利亚特，即为特洛伊，是迈锡尼东北角的小亚细亚的城邦。特洛伊有一个王子帕里斯，出访斯巴达，看上了斯巴达国王墨涅拉奥斯的妻子海伦，在爱神阿佛洛狄忒的帮助下，诱拐了海伦。墨涅拉俄斯十分愤怒，找到了自己的哥哥迈锡尼城邦的国王阿伽门农。阿伽门农号令周围的希腊城邦攻打特洛伊，运用奥德修斯的木马计把城门撞开，抢回海伦，凯旋而归。&lt;/li>
&lt;li>作品内容：&lt;strong>并没详述整个来龙去脉&lt;/strong>，是从战争第十年后发生的一个插曲（阿喀琉斯的愤怒）。希腊联军第一勇士阿喀琉斯在战场上获得一个女俘虏，之后与阿伽门农发生冲突，阿伽门农要将女俘虏占为己有。阿喀琉斯一怒之下退出战争，希腊联军面对以大王子赫克托耳（特洛伊第一勇士，特洛伊方的统帅）为首的特洛伊军队节节败退。阿喀琉斯的好朋友特洛克罗斯盗用了阿喀琉斯的盔甲假扮他出战，被赫克托耳杀死阵前。悲痛愤怒的阿喀琉斯重新出战，击杀了不可战胜的赫克托耳。最后特洛伊国王普里阿摩斯劝说阿喀琉斯动恻隐之心，求得了儿子尸体，特洛伊人民为其举行了盛大的葬礼。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>《伊利亚特》（Iliad）：
犹如凶煞星忽然从浓云里呈现出来，
光亮闪烁，忽而又隐进昏暗的云层里；
赫克托耳也这样一会儿出现在队伍的最前列，
一会儿有隐进队伍的后列里，向他们训令。
他一身铜装，犹如提大盾的父神宙斯的闪电……&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Greek是罗马占领希腊后的称呼，希腊人自称为Hellas，即海伦&lt;/p>
&lt;/blockquote>
&lt;h3 id="奥德修纪">《奥德修纪》&lt;/h3>
&lt;p>特洛伊战争之后英雄回乡的故事。全文采用了倒叙的手法，奥德修斯在归乡途中的第十年来到了一个岛，向岛上的国王讲述了他这十年中经历的逸闻轶事。&lt;/p>
&lt;blockquote>
&lt;p>其中的食荷国的莲蓬，吃了会使人忘记家乡。
风神送给他们一个口袋，可以把所有的逆风都装进去，这样便能一帆风顺回家了。不料当船快行驶到家时，众水手以为口袋里面装的是金银财宝，乘奥德修斯睡觉时打开了口袋，结果各路风神倾刻呼啸而至，又把他们吹到风神岛。
奥德修斯还用烧红的木棍战胜了独目巨人，藏在羊的肚子底下出逃。
来到女妖克尔克的岛上，克尔克将他的同伴变成了猪。在智慧女神的帮助下，战胜了女妖。
游历了地狱，看到了阿喀琉斯的幽灵。
途中遇到塞壬女妖，她用美妙的歌声引诱水手触礁，让水手们用蜡堵住耳朵，并命人把自己绑在桅杆上，最终化险为夷。
在日神岛上，同伴宰食神牛，激怒宙斯，被雷电劈毁船只。
冲到卡吕普索的岛上，并且被软禁了七年。&lt;/p>
&lt;/blockquote>
&lt;p>最后岛主被故事打动，帮助他回到家乡伊大卡岛。回到家乡后发现有许多纨绔子弟觊觎他的王位与坚贞的妻子珀涅罗珀，奥德修斯与儿子忒勒马科斯通过种种计谋杀死心怀不轨的人，一家团圆。&lt;/p>
&lt;h3 id="系统叙事诗">系统叙事诗&lt;/h3>
&lt;p>以神话和英雄传说为内容，填补了荷马史诗和赫西俄德神谱没有提及的地方，将分散凌乱的希腊民间神话传说联系为彼此相关的有机体系。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>《诶塞俄比亚英雄》阿喀琉斯之死&lt;/strong>：阿喀琉斯的母亲是海洋女神，他出生时母亲倒提着他在冥河里泡过，全身刀枪不入，唯一的弱点是后脚跟。最后被花花公子帕里斯从背后偷袭，射中了后脚跟而死。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>《塞浦路斯之歌》三女神的金苹果之争和特洛伊战争的起源&lt;/strong>：阿喀琉斯的母亲海洋女神特提斯与其父亲特萨利亚国王珀琉斯结婚时，请来各路英豪来观礼，却唯独忘了纷争女神。愤怒的纷争女神就在宴会上投下了一个金苹果，上面写着：献给最美丽的女神。与会的女神都认为自己最美，其中就有全身雪白的赫拉、智慧女神女战神雅典娜、美神阿佛洛狄特，三人尤其不甘心，就找上宙斯。宙斯是雅典娜和维纳斯的父亲，赫拉的丈夫，就建议她们出城去找放牧的少年帕里斯评判。 帕里斯因其在出生时母亲梦到特洛伊有血光之灾而被抛弃，三个女神找上他，分别许以最大的城邦、最强大的力量、最美的女人，帕里斯选择了美女。后来美神就让国王重新召回儿子，并引导帕伊斯来到斯巴达抢走海伦，由此引发特洛伊战争。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>《小伊利亚特》《特洛伊失陷记》木马计&lt;/strong>：特洛伊守城十年，希腊联军久攻不下，奥德修斯于是想出了木马计。他命令联军佯装撤退，留下巨大的中空的装有希腊战士的木马在海滩上，特洛伊军队以为自己守城胜利，将木马作为战利品带回城中欢庆。等到夜深人静，木马肚中的战士与重返的联军里应外合，攻下了特洛伊。&lt;/p>
&lt;/blockquote>
&lt;h2 id="赫西俄德和神谱">赫西俄德和《神谱》&lt;/h2>
&lt;h3 id="神谱地位与意义">《神谱》地位与意义&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>独特贡献
与荷马史诗偏重故事性和英雄传说相比，赫西俄德通过游吟，讲述、理清了神的谱系，即神与神的血缘关系，使神的来历变得更加清楚，形成系统。希腊人眼里，神与神的后裔是神，神与人的后裔是英雄，这些英雄往往又是贵族家族的始祖，从家族谱系到英雄谱系，再到诸神的谱系。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>文化学意义
（1）通过神系的生殖原则反映了一种朴素的宇宙起源论和自然演化观，即在科学与哲学产生之前的希腊超越道德的富有童趣的对自然与社会的看法。最早的大地之神、天宇之神代表最原始的自然观，后来出现了美神、商神、战神反应了自然到社会的演变。
（2）蕴含着一种以“命运”为动力的社会进化思想。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="神谱谱系">《神谱》谱系&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>混沌（chaos）中产生了大地盖亚、爱神艾洛斯、地狱塔尔塔罗斯、黑暗和黑夜、白天和光明；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>未经交配，盖亚生下了与她一样宽广的天宇之神乌兰诺斯与海神蓬托斯；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>盖亚与乌兰诺斯相结合，生下了顶天立地的一批神族泰坦神族与巨人族；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>盖亚与乌兰诺斯最小的儿子克罗诺斯与姐姐瑞亚相结合，生下了奥林匹斯诸神；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>奥林匹斯诸神包括：1）克洛诺斯与瑞亚所生的三男三女，农神德墨忒尔、赫拉、灶神赫斯提亚（女）、海神波塞冬、地狱之神普路同（哈德斯）、宙斯；2）宙斯的后代：太阳神阿波罗、战神阿瑞斯、雅典娜、阿佛洛狄特等等&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>泰坦神族&lt;/strong>：泰坦即高大，也有神经质的意思。乌兰诺斯得知他的儿子中将会有一人代替他，所以将所生十二个儿子都打到地狱里去，最小的一个儿子克洛诺斯起来反抗，在盖亚的庇护下阉割了乌兰诺斯，解放了兄弟姐妹。（泰坦尼克号就是大的意思）&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ol>
&lt;h3 id="神谱悲剧">《神谱》悲剧&lt;/h3>
&lt;ol>
&lt;li>乌兰诺斯的悲剧&lt;/li>
&lt;li>克洛诺斯的悲剧：克洛诺斯得到了和乌兰诺斯一样的提示，于是将自己的所有子女在出生时就吞食掉，瑞亚为了躲避他这种蛮横的行为，通过盖亚的帮助躲在小岛上生下了宙斯，将装有石头的襁褓给了克洛诺斯吞食。宙斯长大后变得更加强大，找到了一种神药让克洛诺斯呕吐，将兄弟姐妹都呕吐出来，由此发生泰坦神族占据南方山头与在北方的奥林匹斯神族的战斗，即北方民族与南方民族的战斗。&lt;/li>
&lt;/ol>
&lt;h3 id="神谱后续">《神谱》后续&lt;/h3>
&lt;ol>
&lt;li>《普罗米修斯三部曲》：希腊悲剧之父埃斯库罗斯作先知普罗米修斯三部曲，第一部为《被缚的普罗米修斯》，后两部亡佚，内容保存。&lt;/li>
&lt;li>《被缚的普罗米修斯》：宙斯获胜后同样获得提示，但是他并没有像前代统治者一样采用残暴的方式，而是依然和平相处。这被后世哲学家们分析为专制社会向民主的过度。唯一知道的是先知普罗米修斯，宙斯将他捆绑在高加索的山上，白天要恶鹰啄食肝脏，晚上又修复。&lt;/li>
&lt;li>亡佚内容：普罗米修斯妥协，告知宙斯与海洋女神忒提斯的子女将不可战胜，并且海洋女神与任何神的结合都将不可战胜，于是宙斯将忒提斯强行嫁给英雄珀琉斯，生出的小孩即是阿喀琉斯。(普罗米修斯的妥协即是诶斯库罗斯的妥协。 )&lt;/li>
&lt;/ol>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>荷马史诗、系统叙事诗和《神谱》不仅把散乱的希腊神话和英雄传说连缀为一个有机的体系，而且把某些英雄的家族历史和不幸遭遇当做叙述的主题，开始突出命运和神谕的重要性，从而具有了最初的悲剧色彩，使后来的人们开始关注背后的命运，引出了形而上的&lt;strong>希腊哲学&lt;/strong>。&lt;/p></content><category scheme="https://yinyajun.github.io/categories/%E5%8E%86%E5%8F%B2/" term="历史" label="历史"/><category scheme="https://yinyajun.github.io/tags/%E5%8F%A4%E5%B8%8C%E8%85%8A/" term="古希腊" label="古希腊"/></entry><entry><title type="text">爱琴文明-从克里特到迈锡尼</title><link rel="alternate" type="text/html" href="https://yinyajun.github.io/posts/history/greek01/"/><id>https://yinyajun.github.io/posts/history/greek01/</id><updated>2025-01-01T21:07:29+08:00</updated><published>2019-07-03T21:57:10+00:00</published><author><name>雅俊</name><uri>https://yinyajun.github.io/</uri><email>skyblueice234@gmail.com</email></author><summary type="html">Mooc上武汉大学哲赵林老师主讲的《西方文化概论》之古希腊文化部分的13讲课程。本篇对应于1-2讲。</summary><content type="html">&lt;h2 id="希腊文化">希腊文化&lt;/h2>
&lt;p>希腊文化是西方文化的摇篮，而希腊文化本身也经历了漫长的发展。从希腊文化的开端克里特文明到希腊城邦文明再到希腊化时期文明也经历了2000多年时间。现在所说的希腊文化一般特指希腊城邦文明。在希腊城邦文明之前还有一个更为古老的&lt;strong>爱琴文明&lt;/strong>，它分为两个阶段：&lt;strong>克里特文明&lt;/strong>和&lt;strong>迈锡尼文明&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>克里特文明（源头在南面克里特岛）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>迈锡尼文明（希腊本土东北角的一个城邦）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>爱琴海文明&lt;/p>
&lt;/li>
&lt;li>
&lt;p>希腊城邦文明&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="克里特文明">克里特文明&lt;/h2>
&lt;p>爱琴文明第一个阶段，西方文化发源地。&lt;/p>
&lt;h3 id="一克里特岛地理位置">一、克里特岛地理位置&lt;/h3>
&lt;p>位于地中海东部中央，离东部小亚细亚（土耳其境内），南埃及，西亚（腓尼基，耶路撒冷），北巴尔干半岛（希腊本土）距离大致相等。&lt;/p>
&lt;p>而希腊位于巴尔干半岛，东边为地中海中的爱琴海，西边是亚平宁半岛（意大利半岛，亚德里亚海），再西边是伊比利亚半岛（西班牙半岛）。&lt;/p>
&lt;h3 id="二克里特文明发展">二、克里特文明发展&lt;/h3>
&lt;p>在公元前2500左右，由于受埃及文明的影响和少许两河流域文明影响，开始出现文字。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>两河流域文明&lt;/strong>，又叫美索不达米亚文明，是指在底格里斯河和幼发拉底河两河流域之间的美索不达米亚平原（现今伊拉克境内）所发展出来的文明，是人类最早的文明。尼罗河流域的埃及文明对克里特文明影响较大，克里特人也比较接近埃及人。&lt;/p>
&lt;/blockquote>
&lt;p>在公元前19世纪时，已有将近十万人居住，十分繁荣，又接受希腊本土文明（北方游牧民族）的影响，文明大放异彩；&lt;/p>
&lt;p>在公元前17世纪时，出现了以克诺索斯（克里特岛的中心）为首都的王国，被称为米诺斯王国，进入鼎盛时期。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>米诺斯王朝故事&lt;/strong>：相传住在奥林匹斯山的希腊神话主神宙斯来到了西亚，看上了一个推罗国王的女儿欧罗巴。由于妻子赫拉善妒，宙斯变为一只公牛驮着欧罗巴来到了克里特，生下的儿子建立了米诺斯王国。后来整个欧洲以欧罗巴起名的。&lt;/p>
&lt;/blockquote>
&lt;p>由伊文思考古所得：米诺斯迷宫、精美壁画、“逗牛”传统（由埃及传入到克里特到希腊到罗马，现成为西班牙的独特传统）、1600块线性文字A的泥板，至今无法破译&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>米诺斯王宫的来源&lt;/strong>:克里特岛的国王米诺斯，得罪了海神波塞冬，海神就降下了一头神牛，让神牛与米诺斯的妻子私通，生下了一个人身牛头的怪物米诺陶诺斯。这让米诺斯丢尽了脸。为了遮丑，他请代达罗斯修建了一座迷宫，把这个牛头怪关在里面，不让它出来，也不让别人进去。可是，这头怪物每隔9年就要吃7对童男童女，由当时臣服于米诺斯的希腊城邦雅典进贡。&lt;/p>
&lt;/blockquote>
&lt;p>公元前1500,米诺斯文明毁灭，原因不明。普遍认为是：&lt;/p>
&lt;ol>
&lt;li>地质灾害：火山爆发。&lt;/li>
&lt;li>人祸：来自北方的阿卡亚人（印欧语系，而克里特人被认为是属于闪米特语系）的暴力入侵。&lt;/li>
&lt;/ol>
&lt;h2 id="迈锡尼文明">迈锡尼文明&lt;/h2>
&lt;p>爱琴文明第二个阶段。&lt;/p>
&lt;h3 id="一地理位置">一、地理位置&lt;/h3>
&lt;p>位于巴尔干半岛南部的伯罗奔尼撒半岛（像树叶）中央。&lt;/p>
&lt;h3 id="二文明发展">二、文明发展&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>文明发掘：由现代考古学之父施里曼发掘&lt;/p>
&lt;/li>
&lt;li>
&lt;p>风格特点：阿卡亚人（Achaean）在公元前1500毁灭了克里特文明之后取而代之成为文明中心，在文化上对克里特文明多有借鉴，但由于本身是野蛮的北方游牧民族，所以在风格方面与克里特文明迥然而异。以巨大建筑著称：希腊神话中的独目巨人三兄弟，由盖亚所生，库克罗比。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>人种特点：据后世考古，克里特人更偏向为埃及人，而&lt;strong>阿卡亚人则是最早的希腊人&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>荷马史诗《&lt;strong>伊利亚特iliad&lt;/strong>》中将希腊人称为阿卡亚人。比如史诗的开篇：歌唱吧，女神，歌唱珀琉斯（Peleus）之子阿喀琉斯（Achilles）的愤怒，这愤怒给阿卡亚人带来了无限的苦难。&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>文明成就：&lt;/p>
&lt;ul>
&lt;li>狮子门、厚几米到十几米的库克罗比城墙；&lt;/li>
&lt;li>以宙斯为首的奥林匹斯神话：父系社会的男神崇拜取代了米诺斯的女神崇拜，喜好穷兵黩武、吃喝玩乐，不再从事生产，代表征服者的形象；&lt;/li>
&lt;li>由线性文字A发展而来线性文字B。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>文明毁灭：公元前1200，被来自北方更加野蛮的民族多利亚人(Dorian)毁灭，废弃线性文字B，从此不被人记起。从此以后，文字毁灭，希腊半岛陷入了长达三百多年的“黑暗时代”，文明大倒退，文化水平极低。&lt;/p>
&lt;p>人民由于黑暗的统治开始大迁徙，文化重新洗牌。其中伊特鲁里亚人迁徙到了意大利中部，被认为是罗马文明的开创者。&lt;/p>
&lt;p>“黑暗时代”又被称为“英雄时代”，因为产生了一批荷马这样的游吟诗人，通过游吟传唱的方式，记述了黑暗时代之前的迈锡尼文明，记录了阿卡亚英雄，将迈锡尼文明传给了后来的希腊城邦文明。
&lt;strong>公元前776年，第一届奥林匹亚竞技会开始，从此成为走出黑暗时代的标志。&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="三-爱琴文明发展简表">三、 爱琴文明发展简表&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">时间&lt;/th>
&lt;th style="text-align: left">文明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">BC26-15&lt;/td>
&lt;td style="text-align: left">优雅的克里特人建立的米诺斯文明&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">BC16-12&lt;/td>
&lt;td style="text-align: left">豪迈的阿卡亚人建立的迈锡尼文明&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">BC12-8&lt;/td>
&lt;td style="text-align: left">野蛮的多利亚人入侵导致的黑暗时代&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></content><category scheme="https://yinyajun.github.io/categories/%E5%8E%86%E5%8F%B2/" term="历史" label="历史"/><category scheme="https://yinyajun.github.io/tags/%E5%8F%A4%E5%B8%8C%E8%85%8A/" term="古希腊" label="古希腊"/></entry><entry><title type="text">有趣的TensorSlow(下)</title><link rel="alternate" type="text/html" href="https://yinyajun.github.io/posts/ai/basic/tensorslow02/"/><id>https://yinyajun.github.io/posts/ai/basic/tensorslow02/</id><updated>2025-01-01T21:07:29+08:00</updated><published>2019-06-02T22:37:36+00:00</published><author><name>雅俊</name><uri>https://yinyajun.github.io/</uri><email>skyblueice234@gmail.com</email></author><summary type="html">上篇了解了如何用TensorSlow构建计算图并完成前向传播，这里将继续了解计算损失和利用反向传播更新模型参数。</summary><content type="html">&lt;p>上篇了解了如何用TensorSlow构建计算图并完成前向传播，这里将继续了解计算损失和利用反向传播更新模型参数。&lt;/p>
&lt;h2 id="tensorslow简介">TensorSlow简介&lt;/h2>
&lt;p>TensorSlow是极简的模仿TensorFlow的API的python包。&lt;/p>
&lt;blockquote>
&lt;p>本文是对原作者danielsabinasz的&lt;a href="http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/">教程&lt;/a>的基础上，添加了一点自己的理解。&lt;/p>
&lt;p>TensorSlow &lt;a href="https://github.com/danielsabinasz/TensorSlow">&lt;strong>Github repo地址&lt;/strong>&lt;/a>
TensorSlow原作者&lt;a href="http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/">&lt;strong>英文教程&lt;/strong>&lt;/a>&lt;/p>
&lt;p>&lt;em>The source code has been built with maximal understandability in mind, rather than maximal efficiency.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="perceptron-example">Perceptron Example&lt;/h2>
&lt;p>本文中将会使用TensorSlow处理稍微复杂一点的模型。首先利用上篇的代码，搭建一个表征Perceptron的计算图。后面将以这个Perceptron为例，介绍TensorSlow如何进行损失计算和反向传播的。值得一提的是，这里的输入和参数已经是向量。&lt;/p>
&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d321648b86553eea745db.png" alt="Perceptron计算图">&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">Graph&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">as_default&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create a weight matrix for 2 output classes:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># One with a weight vector (1, 1) for blue and &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># one with a weight vector (-1, -1) for red&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">([&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">session&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">output_probabilities&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">concatenate&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">red_points&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">})&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="反向传播">反向传播&lt;/h2>
&lt;p>上面构建的计算图，能够实现了Perceptron模型的前向传播。目前为止，模型的参数都是初值，已经给定的。这些仅仅为初值的参数效果好吗？那么需要用&lt;strong>损失函数&lt;/strong>来评价当前参数。&lt;/p>
&lt;h3 id="损失函数">损失函数&lt;/h3>
&lt;p>对于二分类问题而言，最大似然估计(MLE)通常是比较好的参数估计的选择。MLE的损失函数形式是负对数似然，也就是交叉熵:&lt;/p>
&lt;p>$$J = - \sum_{i=1}^N \sum_{j=1}^C c_{i, j} \cdot log p_{i, j}$$&lt;/p>
&lt;p>其中，$p_{ij}$代表模型对第$i$个样本预测是第$j$个类别的分数，$c_{ij}$代表第$i$个样本是否是第$j$个类别。同样，损失函数$J$也对应于计算图中的一个operation节点：&lt;/p>
&lt;!--
![添加损失op](https://raw.githubusercontent.com/yinyajun/yinyajun.github.io/master/images/figure/loss_graph.png)
-->
&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d321648b86553eea745d7.png" alt="添加损失op">&lt;/p>
&lt;p>怎么建立这个operation节点？在原计算图的基础上，可以将节点$J$写成这样：
$$\sum_{i=1}^N \sum_{j=1}^C (c \odot log , p)_{i, j}$$&lt;/p>
&lt;p>其中，$c=[c_{i1},\dots,c_{iC}]$是第$i$个样本的label的one-hot向量。可以发现，这是由多个基本的operation节点组成的。实现下面这些基础的operation节点并加以组合，即可得到节点$J$。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">节点&lt;/th>
&lt;th style="text-align: left">描述&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">$log$ 节点&lt;/td>
&lt;td style="text-align: left">The element-wise logarithm&lt;br> of a matrix or vector&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">$\odot$ 节点&lt;/td>
&lt;td style="text-align: left">The element-wise product&lt;br> of two matrices&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">$\sum_{j=1}^C$ 节点&lt;/td>
&lt;td style="text-align: left">Sum over the&lt;br>columns of a matrix&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">$\sum_{i=1}^N$ 节点&lt;/td>
&lt;td style="text-align: left">Sum over the&lt;br>rows of a matrix&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">$-$ 节点&lt;/td>
&lt;td style="text-align: left">Taking the negative&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>基础的operation节点写法在上篇已有介绍，这里仅以log节点为例，代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Operation&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Computes the natural logarithm of x element-wise.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">compute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x_value&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_value&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>最后，组合这些operation节点，可以完整的给出节点$J$并计算出loss：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create a new graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Graph&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">as_default&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">([&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Cross-entropy loss&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">J&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">negative&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">reduce_sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">reduce_sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">multiply&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">session&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">J&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">concatenate&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">red_points&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">+&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">red_points&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="梯度下降">梯度下降&lt;/h3>
&lt;p>上面的步骤已经计算出模型损失函数。损失函数越小，意味着该模型参数下，模型输出和真实标签越接近。而搜索模型参数，使损失函数最小的方法叫做&lt;strong>梯度下降&lt;/strong>，流程如下&lt;/p>
&lt;blockquote>
&lt;ol>
&lt;li>参数$W$和$b$设置随机初始值。&lt;/li>
&lt;li>计算$J$对$W$和$b$的梯度。&lt;/li>
&lt;li>分别在其负梯度的方向上下降一小步(由&lt;code>learning_rate&lt;/code>控制步长)。&lt;/li>
&lt;li>回到step 2，继续执行，直到收敛。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>而&lt;code>GradientDescentOptimizer&lt;/code>就实现了上述流程中的step 3：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">GradientDescentOptimizer&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">learning_rate&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">learning_rate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">learning_rate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">minimize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">learning_rate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">learning_rate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">class&lt;/span> &lt;span class="nc">MinimizationOperation&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Operation&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">compute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad_table&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">compute_gradients&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Iterate all variables&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">type&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Take a step along the direction of the negative gradient&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">value&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="n">learning_rate&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">grad&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">MinimizationOperation&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>上述代码中&lt;code>compute_gradients&lt;/code>函数对应于step 2，将在下一个小节介绍。&lt;code>grad_table&lt;/code>这个字典存放了损失函数$J$节点对图中所有Variable节点的当前梯度（因为只有Variable节点才需要更新）。&lt;/p>
&lt;h3 id="梯度计算">梯度计算&lt;/h3>
&lt;p>梯度的计算根据导数的&lt;strong>链式法则&lt;/strong>。&lt;/p>
&lt;h4 id="链式法则">链式法则&lt;/h4>
&lt;p>链式法则是微积分中的求导法则，用于求一个复合函数的导数。示例如下：&lt;/p>
&lt;!--
![链式法则](https://raw.githubusercontent.com/yinyajun/yinyajun.github.io/master/images/figure/abcde2.png)
-->
&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d311f48b86553eea6f991.png" alt="链式法则">
$$
\begin{aligned}
\frac{\partial e}{\partial a}
&amp;amp;= \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial a}\\
&amp;amp;= \frac{\partial e}{\partial d} \cdot \left( \frac{\partial d}{\partial b} \cdot \frac{\partial b}{\partial a} + \frac{\partial d}{\partial c} \cdot \frac{\partial c}{\partial a} \right)\\
&amp;amp;= \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial b} \cdot \frac{\partial b}{\partial a} + \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial c} \cdot \frac{\partial c}{\partial a}
\end{aligned}
$$&lt;/p>
&lt;h3 id="梯度计算-1">梯度计算&lt;/h3>
&lt;p>通过上面的链式法则，计算损失函数节点（记为loss节点）对当前节点$n$的梯度，也是链式的：&lt;/p>
&lt;ol>
&lt;li>计算loss节点对当前$n$节点的consumer节点的输出的梯度$G$.&lt;/li>
&lt;li>计算$n$节点的consumer节点对$n$节点梯度$\times G$，将所有consumer节点计算出的梯度全部相加。&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code># 计算当前节点梯度的伪代码
grad_n = grad_child1 * grad_child1_n + ...
+ grad_childk * grad_childk_n
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>&lt;code>grad_n&lt;/code> : $\partial\text{loss}/\partial{n}$&lt;/li>
&lt;li>&lt;code>chlid1&lt;/code>: a child node of node $n$&lt;/li>
&lt;li>&lt;code>grad_child1&lt;/code>: $\partial\text{loss}/\partial{\text{child1}}$&lt;/li>
&lt;li>&lt;code>grad_child1_n&lt;/code>: $\partial\text{child1}/\partial{n}$&lt;/li>
&lt;/ul>
&lt;p>先计算子节点梯度，然后得到当前节点梯度，完全按照链式法则。由于当前节点的梯度计算依赖子节点的梯度计算，因此可以使用拓扑排序。而作者使用了BFS来完成拓扑排序。从反图角度而言，每个节点的入度都为0，可以放心使用BFS来完成拓扑排序。&lt;/p>
&lt;p>通过一步步迭代，能够得到所有节点的梯度。其中$\partial\text{child1}/\partial{n}$，也就是子节点的输出对于该节点的输出的梯度如何计算？这个工作应该由子节点来完成。&lt;/p>
&lt;p>对于一个operation节点，提前定义这个operation的梯度计算函数，并使用装饰器&lt;code>@RegisterGradient&lt;/code>来将实现了计算梯度函数的opeartion节点注册到全局变量&lt;code>_gradient_registry&lt;/code>中，具体细节见下一小节。整体的梯度计算过程如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">queue&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Queue&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_gradients&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad_table&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Perform a breadth-first search, backwards from the loss&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">set&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Queue&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">put&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">queue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">queue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Iterate all consumers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">consumer&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">consumers&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lossgrad_wrt_consumer_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">consumer&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Retrieve the function which computes gradients with respect to&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># consumer&amp;#39;s inputs given gradients with respect to consumer&amp;#39;s output.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">consumer_op_type&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">consumer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="vm">__class__&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bprop&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">_gradient_registry&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">consumer_op_type&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Get the gradient of the loss with respect to all of consumer&amp;#39;s inputs&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lossgrads_wrt_consumer_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">bprop&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">consumer&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lossgrad_wrt_consumer_output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">consumer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_nodes&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># If there is a single input node to the consumer, lossgrads_wrt_consumer_inputs is a scalar&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">lossgrads_wrt_consumer_inputs&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Otherwise, lossgrads_wrt_consumer_inputs is an array of gradients for each input node&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node_index_in_consumer_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">consumer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_nodes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Get the gradient of the loss with respect to node&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lossgrad_wrt_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">lossgrads_wrt_consumer_inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node_index_in_consumer_inputs&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grad_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">lossgrad_wrt_node&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">hasattr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;input_nodes&amp;#34;&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">input_node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_nodes&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">input_node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">put&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">grad_table&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中一些细节还需要自己体会代码。&lt;/p>
&lt;h3 id="operation节点的梯度">Operation节点的梯度&lt;/h3>
&lt;p>根据operation操作的&lt;code>compute&lt;/code>方法给出节点正向传播的函数，据此计算梯度函数，然后注册到全局变量中即可。&lt;/p>
&lt;p>以矩阵乘法&lt;code>matmul&lt;/code>为例：给定对于$AB$的梯度$G$，其对于$A$的梯度是$GB^T$，对于$B$的梯度是$A^TG$，所以该节点的梯度是一个向量。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="nd">@RegisterGradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;matmul&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">_matmul_gradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">op&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">grad&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">A&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">op&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">B&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">op&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">grad&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grad&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>同样的&lt;code>sigmoid&lt;/code>的梯度可以写为$G \cdot \sigma(a) \cdot \sigma(1-a)$&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="nd">@RegisterGradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;sigmoid&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">_sigmoid_gradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">op&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">grad&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sigmoid&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">op&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">grad&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sigmoid&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">sigmoid&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="perceptron-train">Perceptron Train&lt;/h2>
&lt;p>完整的Perceptron模型训练代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create a new graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Graph&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">as_default&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Initialize weights randomly: step 1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">J&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">negative&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">reduce_sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">reduce_sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">multiply&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># step 2 and step 3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">minimization_op&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">GradientDescentOptimizer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">learning_rate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.01&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">minimize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">J&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">feed_dict&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">concatenate&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">red_points&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">+&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">red_points&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">session&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Perform 100 gradient descent steps, step 4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">step&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">100&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">J_value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">J&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">step&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Step:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">step&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34; Loss:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">J_value&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">minimization_op&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">W&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Weight matrix:&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W_value&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b_value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Bias:&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b_value&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>Step: 0 Loss: 202.782788396
Step: 10 Loss: 4.04566479054
Step: 20 Loss: 2.69644468305
Step: 30 Loss: 2.00506261735
Step: 40 Loss: 1.62202006027
Step: 50 Loss: 1.39268559111
Step: 60 Loss: 1.24498439759
Step: 70 Loss: 1.14348265257
Step: 80 Loss: 1.06965484385
Step: 90 Loss: 1.01324253829
Weight matrix:
[[ 1.27496197 -1.77251219]
[ 1.11820232 -2.01586474]]
Bias:
[-0.45274057 -0.39071841]
&lt;/code>&lt;/pre>
&lt;p>可以发现loss的确是不断降低的，模型参数不断优化更新。&lt;/p>
&lt;h2 id="multi-layer-perceptrons">Multi-Layer Perceptrons&lt;/h2>
&lt;p>使用TensorSlow来处理更加复杂的问题：分类的决策边界更加复杂。这里也搭建了更加复杂的模型MLP。&lt;/p>
&lt;h3 id="数据分布">数据分布&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">matplotlib.pyplot&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">plt&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">tensorslow&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">ts&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create two clusters of red points centered at (0, 0) and (1, 1), respectively.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">red_points&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">concatenate&lt;/span>&lt;span class="p">((&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mf">0.2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">25&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">25&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mf">0.2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">25&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">25&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create two clusters of blue points centered at (0, 1) and (1, 0), respectively.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">blue_points&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">concatenate&lt;/span>&lt;span class="p">((&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mf">0.2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">25&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">25&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mf">0.2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">25&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">25&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Plot them&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">red_points&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">red_points&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">color&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;red&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">blue_points&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">color&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;blue&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d393a48b86553eea9d311.png" alt="真实数据分布">&lt;/p>
&lt;h3 id="计算图">计算图&lt;/h3>
&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d393a48b86553eea9d313.png" alt="MLP计算图">&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create a new graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Graph&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">as_default&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create training input placeholder&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create placeholder for the training classes&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build a hidden layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_hidden1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b_hidden1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p_hidden1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sigmoid&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W_hidden1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b_hidden1&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build a hidden layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_hidden2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">8&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b_hidden2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">8&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p_hidden2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sigmoid&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p_hidden1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W_hidden2&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b_hidden2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build a hidden layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_hidden3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">8&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b_hidden3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p_hidden3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sigmoid&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p_hidden2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W_hidden3&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b_hidden3&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build the output layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p_hidden3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W_output&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">b_output&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build cross-entropy loss&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">J&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">negative&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reduce_sum&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reduce_sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">multiply&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p_output&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build minimization op&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">minimization_op&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">train&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">GradientDescentOptimizer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">learning_rate&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.03&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">minimize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">J&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Build placeholder inputs&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">feed_dict&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">concatenate&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">red_points&lt;/span>&lt;span class="p">)),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">blue_points&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">+&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">red_points&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create session&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">session&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ts&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Perform 100 gradient descent steps&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">step&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2000&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">J_value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">J&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">step&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">100&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Step:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">step&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34; Loss:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">J_value&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">minimization_op&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>Step: 0 Loss: 105.25316761015766
Step: 100 Loss: 54.82276887616324
Step: 200 Loss: 18.531741905961816
Step: 300 Loss: 10.88319073941583
Step: 400 Loss: 5.167908735173651
Step: 500 Loss: 4.015258056948531
...
Step: 1400 Loss: 0.1973992659737359
Step: 1500 Loss: 0.17700496511514013
Step: 1600 Loss: 0.1602628699213534
Step: 1700 Loss: 0.14629349101006833
Step: 1800 Loss: 0.13447502395360536
Step: 1900 Loss: 0.1243562427509077
&lt;/code>&lt;/pre>
&lt;h3 id="可视化">可视化&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Visualize classification boundary&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">xs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ys&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">pred_classes&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">xs&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">ys&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pred_class&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p_output&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">feed_dict&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">]]})[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pred_classes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pred_class&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">()))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">xs_p&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ys_p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[],&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">xs_n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ys_n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[],&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">pred_classes&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xs_n&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ys_n&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xs_p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ys_p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xs_p&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ys_p&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;ro&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">xs_n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ys_n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;bo&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d393a48b86553eea9d30f.png" alt="决策边界">&lt;/p>
&lt;h2 id="小结">小结&lt;/h2>
&lt;p>短小精悍的TensorSlow揭示了了深度模型框架的基本工作机理。当然，它只适用于教学，生产环境下的深度学习框架软件将更加复杂。&lt;/p></content><category scheme="https://yinyajun.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" term="人工智能" label="人工智能"/><category scheme="https://yinyajun.github.io/tags/tensorslow/" term="TensorSlow" label="TensorSlow"/></entry><entry><title type="text">有趣的TensorSlow(上)</title><link rel="alternate" type="text/html" href="https://yinyajun.github.io/posts/ai/basic/tensorslow01/"/><id>https://yinyajun.github.io/posts/ai/basic/tensorslow01/</id><updated>2025-01-01T21:07:29+08:00</updated><published>2019-06-01T22:37:36+00:00</published><author><name>雅俊</name><uri>https://yinyajun.github.io/</uri><email>skyblueice234@gmail.com</email></author><summary type="html">TensorFlow的许多概念，如graph, session, operation等，为什么要这么设计？Github上的TensorSlow用纯python来模仿了TF的底层api，加深理解TF中的底层概念。</summary><content type="html">&lt;p>TensorFlow的许多概念，如graph, session, operation等，为什么要这么设计？Github上的TensorSlow用纯python来模仿了TF的底层api，加深理解TF中的底层概念。&lt;/p>
&lt;h2 id="tensorslow简介">TensorSlow简介&lt;/h2>
&lt;ul>
&lt;li>极简的模仿TensorFlow的API的python包。&lt;/li>
&lt;li>使用纯python作为后端。&lt;/li>
&lt;li>仅用作教学目的，帮助理解TensorFlow底层原理。&lt;/li>
&lt;li>代码量非常少，跟着教程走，很容易看完。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>本文是对原作者danielsabinasz的&lt;a href="http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/">教程&lt;/a>
的基础上，添加了一点自己的理解。&lt;/p>
&lt;p>TensorSlow &lt;a href="https://github.com/danielsabinasz/TensorSlow">&lt;strong>Github repo地址&lt;/strong>&lt;/a>
TensorSlow原作者&lt;a href="http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/">&lt;strong>英文教程&lt;/strong>&lt;/a>&lt;/p>
&lt;p>&lt;em>The source code has been built with maximal understandability in mind, rather than maximal efficiency.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="计算图computational-graphs">计算图Computational Graphs&lt;/h2>
&lt;p>计算图是一种有向图，它是以图的形式来表示或计算数学函数。和普通的图一样，计算图中也有节点和边。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>节点&lt;/strong>：要么是提供输入数据的节点，要么是代表操作数据的函数的节点。&lt;/li>
&lt;li>&lt;strong>边&lt;/strong>：函数参数（或者说数据依赖），以流的形式为节点传输数据。&lt;/li>
&lt;li>&lt;strong>Tensor&lt;/strong>: 节点的输入和输出数据，其实就是一个多维的array。&lt;/li>
&lt;/ul>
&lt;p>下图展示了一个计算图，它描述了怎么计算输入节点$x$和$y$的和$z$的过程。&lt;/p>
&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d3a8248b86553eeaa37eb.png" alt="计算图">&lt;/p>
&lt;p>$x$和$y$都是$z$的输入节点，而$z$是$x$和$y$的消费节点。节点$z$描述了这么一个方程：&lt;/p>
&lt;p>$$z:\mathcal{R}^2 \rightarrow \mathcal{R}, z(x,y) = x + y.$$&lt;/p>
&lt;p>计算图这个概念非常有用，特别当计算非常复杂的时候，下面的例子对应于一个仿射变换:
$$z(A,x,b)= Ax+b$$&lt;/p>
&lt;p>&lt;img src="https://pic.downk.cc/item/5e4d3a8248b86553eeaa37ed.png" alt="仿射变换的计算图">&lt;/p>
&lt;p>首先了解各类型节点的表示，从节点输入，节点输出和节点操作来考察各类型节点。&lt;/p>
&lt;h3 id="operations节点">Operations节点&lt;/h3>
&lt;p>每个operation节点以下面三个表征：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>节点操作&lt;/strong>：当operation节点的输入的值给定时，用&lt;code>compute&lt;/code>函数来计算该operation节点的输出&lt;/li>
&lt;li>&lt;strong>节点输入&lt;/strong>：&lt;code>input_nodes&lt;/code>列表，可以是varibles节点或者是其他operations节点&lt;/li>
&lt;li>&lt;strong>节点输出&lt;/strong>：&lt;code>consumers&lt;/code>列表，它们将该operation节点的输出作为它们的输入。&lt;/li>
&lt;/ul>
&lt;p>上面三个含义都是显而易见的描述operation节点的操作，在&lt;code>Opearation&lt;/code>类中，用三个成员表示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Operation&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">input_nodes&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[]):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_nodes&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">input_nodes&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Initialize list of consumers &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">consumers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Append this operation to the list of consumers of all input nodes&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">input_node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">input_nodes&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">consumers&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Append this operation to the list of operations in the currently active default graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_default_graph&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">operations&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">compute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">pass&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>compute&lt;/code>方法是需要每个operation节点子类去实现。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Addition Operation节点示例&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Operation&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Returns x + y element-wise.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">compute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x_value&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_value&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">x_value&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_value&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">x_value&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">y_value&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Matrix Multiplicaiton Operation节点示例&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Operation&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Multiplies matrix a by matrix b, producing a * b.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">compute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">a_value&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b_value&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">a_value&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b_value&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">a_value&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">b_value&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="placeholder节点">Placeholder节点&lt;/h3>
&lt;p>计算图中为了计算输出，必须要向图中提供一次输入数据。而Placeholder节点，正如其名，就是用来干这事的。在仿射变换计算图的例子中，$x$就是这种节点。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>节点操作&lt;/strong>：无&lt;/li>
&lt;li>&lt;strong>节点输入&lt;/strong>：无&lt;/li>
&lt;li>&lt;strong>节点输出&lt;/strong>：&lt;code>consumers&lt;/code>列表，它们将该节点的输出作为它们的输入。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">placeholder&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Represents a placeholder node that has to be provided with a value
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> when computing the output of a computational graph
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">consumers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_default_graph&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">placeholders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="variables节点">Variables节点&lt;/h3>
&lt;p>在仿射变换的例子中，$x$,$A$和$b$都不是operation节点，但是$x$与$A$和$b$有一些区别，$x$是纯粹的输入placeholder节点，而$A$和$b$是能不断变更输出值，它们是Variable节点。这些Variable节点虽然没有输入，但本身有初值。Variable节点是计算图的固有成分。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>节点操作&lt;/strong>：无&lt;/li>
&lt;li>&lt;strong>节点输入&lt;/strong>：无&lt;/li>
&lt;li>&lt;strong>节点输出&lt;/strong>：&lt;code>consumers&lt;/code>列表，它们将该节点的输出作为它们的输入。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Variable&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Represents a variable (i.e. an intrinsic, changeable parameter of a computational graph).
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">initial_value&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">initial_value&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">consumers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_default_graph&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">variables&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="the-graph-class">The Graph class&lt;/h3>
&lt;p>使用&lt;code>Graph&lt;/code>来绑定所有创建的节点。当创建新的graph的时候，可以调用&lt;code>as_default&lt;/code>方法来设置默认图&lt;code>_default_graph&lt;/code>
，这样我们不用显示地去将节点绑定到图中。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Graph&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Construct Graph&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">operations&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">placeholders&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">variables&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">as_default&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">global&lt;/span> &lt;span class="n">_default_graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_default_graph&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="example">Example&lt;/h3>
&lt;p>通过已经建立的类，来建立仿射变换例子的计算图：&lt;/p>
&lt;p>$$
z = \begin{pmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; -1
\end{pmatrix}
\cdot
x&lt;/p>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>\begin{pmatrix}
1 \\
1
\end{pmatrix}
$$&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create a new graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Graph&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">as_default&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create variables&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create placeholder&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create hidden node y&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create output node z&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">z&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="session">Session&lt;/h3>
&lt;p>建立完计算图，那么开始思考，如何计算出节点的输出？输出节点通常是operation节点。为了正确计算输出节点的输出，需要按正确的顺序计算。仍以仿射变换为例，要计算$z$必须先计算出中间结果$y$。也就是说，
&lt;strong>必须保证节点是按顺序执行的，计算节点$o$之前，节点$o$的所有输入节点已经完成计算&lt;/strong>，使用&lt;strong>拓扑排序&lt;/strong>即可满足要求。&lt;/p>
&lt;p>拓扑排序，是原图的reverse post order，和反图的post order一致。这里使用反图的post order来得到拓扑顺序。这一系列计算都封装在了Session中。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Session&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Represents a particular execution of a computational graph.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">operation&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{}):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nodes_postorder&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">traverse_postorder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">operation&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Iterate all nodes to determine their value&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">nodes_postorder&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">type&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">placeholder&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Set the node value to the placeholder value from feed_dict&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">elif&lt;/span> &lt;span class="nb">type&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Variable&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Set the node value to the variable&amp;#39;s value attribute&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">value&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="c1"># Operation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Get the input values for this operation from node_values&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">input_node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">input_node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_nodes&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Compute the output of this operation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">compute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Convert lists to numpy arrays&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">type&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Return the requested node value&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">operation&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">traverse_postorder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">operation&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nodes_postorder&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">recurse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">isinstance&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Operation&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">input_node&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_nodes&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">recurse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nodes_postorder&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">recurse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">operation&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">nodes_postorder&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可见，&lt;code>run&lt;/code>方法对要计算的operation节点进行了一次拓扑排序，按照这个顺序，依次计算节点。&lt;/p>
&lt;h4 id="example-1">Example&lt;/h4>
&lt;p>完成仿射变换例子的输出.&lt;/p>
&lt;p>$$
z=\begin{pmatrix}1 &amp;amp; 0 \\ 0 &amp;amp; -1 \end{pmatrix} \cdot \begin{pmatrix}1 \\ 2\end{pmatrix} + \begin{pmatrix}
1 \\
1
\end{pmatrix}=
\begin{pmatrix}
2 \\
-1
\end{pmatrix}
$$&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">session&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">})&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>[ 2 -1]
&lt;/code>&lt;/pre>
&lt;h2 id="小结">小结&lt;/h2>
&lt;p>目前，已经可以搭建计算图，用来计算一些复杂函数了。如果用计算图来搭建神经网络，目前的代码完全能够完成网络的前向传播。&lt;a href="https://yinyajun.github.io/infomation-tech/tensorslow-02/">下篇&lt;/a>
将会涉及到loss计算及反向传播在计算图中如何实现。&lt;/p></content><category scheme="https://yinyajun.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" term="人工智能" label="人工智能"/><category scheme="https://yinyajun.github.io/tags/tensorslow/" term="TensorSlow" label="TensorSlow"/></entry></feed>