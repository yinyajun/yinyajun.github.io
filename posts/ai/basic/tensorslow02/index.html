<!doctype html><html lang=zh-CN><head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#"><meta charset=UTF-8><meta name=generator content="Hugo 0.135.0"><meta name=theme-color content="#16171d"><meta name=color-scheme content="light dark"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no, date=no, address=no, email=no"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><title>有趣的TensorSlow(下) | 算迹</title>
<link rel=stylesheet href=/css/meme.min.css><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@2/dist/instantsearch.min.js defer></script><script src=/js/meme.min.js></script><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto+Serif+SC:wght@400;500;700&amp;family=Source+Code+Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" media=print onload='this.media="all"'><noscript><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto+Serif+SC:wght@400;500;700&amp;family=Source+Code+Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap"></noscript><meta name=author content="Yajun"><meta name=description content="上篇了解了如何用TensorSlow构建计算图并完成前向传播，这里将继续了解计算损失和利用反向传播更新模型参数。"><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=mask-icon href=/icons/safari-pinned-tab.svg color=#2a6df4><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-title content="算迹"><meta name=apple-mobile-web-app-status-bar-style content="black"><meta name=mobile-web-app-capable content="yes"><meta name=application-name content="算迹"><meta name=msapplication-starturl content="../../../../"><meta name=msapplication-TileColor content="#fff"><meta name=msapplication-TileImage content="../../../../icons/mstile-150x150.png"><link rel=manifest href=/manifest.json><link rel=canonical href=https://yinyajun.github.io/posts/ai/basic/tensorslow02/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","datePublished":"2019-06-02T22:37:36+00:00","dateModified":"2025-01-01T21:07:29+08:00","url":"https://yinyajun.github.io/posts/ai/basic/tensorslow02/","headline":"有趣的TensorSlow(下)","description":"上篇了解了如何用TensorSlow构建计算图并完成前向传播，这里将继续了解计算损失和利用反向传播更新模型参数。","inLanguage":"zh-CN","articleSection":"posts","wordCount":3378,"image":["https://pic.downk.cc/item/5e4d321648b86553eea745db.png","https://pic.downk.cc/item/5e4d321648b86553eea745d7.png","https://pic.downk.cc/item/5e4d311f48b86553eea6f991.png","https://pic.downk.cc/item/5e4d393a48b86553eea9d311.png","https://pic.downk.cc/item/5e4d393a48b86553eea9d313.png","https://pic.downk.cc/item/5e4d393a48b86553eea9d30f.png"],"author":{"@type":"Person","description":"凡是过往，结尾序章","email":"skyblueice234@gmail.com","image":"https://yinyajun.github.io/icons/apple-touch-icon.png","url":"https://yinyajun.github.io/","name":"Yajun"},"publisher":{"@type":"Organization","name":"算迹","logo":{"@type":"ImageObject","url":"https://yinyajun.github.io/icons/apple-touch-icon.png"},"url":"https://yinyajun.github.io/"},"mainEntityOfPage":{"@type":"WebSite","@id":"https://yinyajun.github.io/"}}</script><meta name=twitter:card content="summary_large_image"><meta property="og:title" content="有趣的TensorSlow(下)"><meta property="og:description" content="上篇了解了如何用TensorSlow构建计算图并完成前向传播，这里将继续了解计算损失和利用反向传播更新模型参数。"><meta property="og:url" content="https://yinyajun.github.io/posts/ai/basic/tensorslow02/"><meta property="og:site_name" content="算迹"><meta property="og:locale" content="zh"><meta property="og:image" content="https://pic.downk.cc/item/5e4d321648b86553eea745db.png"><meta property="og:type" content="article"><meta property="article:published_time" content="2019-06-02T22:37:36+00:00"><meta property="article:modified_time" content="2025-01-01T21:07:29+08:00"><meta property="article:section" content="posts"></head><body><div class=container><header class=header><div class=header-wrapper><div class="header-inner single"><div class=site-brand><a href=/ class=brand>算迹</a></div><nav class=nav><ul class=menu id=menu><li class=menu-item><a href=/categories/><span class=menu-item-name>分类</span></a></li><li class=menu-item><a href=/archives/><span class=menu-item-name>归档</span></a></li><li class=menu-item><a href=/tags/><span class=menu-item-name>标签</span></a></li><li class=menu-item><a href=/about/><span class=menu-item-name>关于</span></a></li><li class=menu-item><a id=theme-switcher href=#><svg viewBox="0 0 512 512" class="icon theme-icon-light"><path d="M193.2 104.5 242 7a18 18 0 0128 0l48.8 97.5L422.2 70A18 18 0 01442 89.8l-34.5 103.4L505 242a18 18 0 010 28l-97.5 48.8L442 422.2A18 18 0 01422.2 442l-103.4-34.5L270 505a18 18 0 01-28 0l-48.8-97.5L89.8 442A18 18 0 0170 422.2l34.5-103.4-97.5-48.8a18 18 0 010-28l97.5-48.8L70 89.8A18 18 0 0189.8 70zM256 128a128 128 0 10.01.0M256 160a96 96 0 10.01.0"/></svg><svg viewBox="0 0 512 512" class="icon theme-icon-dark"><path d="M27 412A256 256 0 10181 5a11.5 11.5.0 00-5 20A201.5 201.5.0 0142 399a11.5 11.5.0 00-15 13"/></svg></a></li><li class="menu-item search-item"><form id=search class=search role=search><label for=search-input><svg viewBox="0 0 512 512" class="icon search-icon"><path d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></label>
<input type=search id=search-input class=search-input></form><template id=search-result hidden><article class="content post"><h2 class=post-title><a class=summary-title-link></a></h2><summary class=summary></summary><div class=read-more-container><a class=read-more-link>阅读更多 »</a></div></article></template></li></ul></nav></div></div></header><main class="main single" id=main><div class=main-inner><article class="content post h-entry" data-align=justify data-type=posts data-toc-num=true><h1 class="post-title p-name">有趣的TensorSlow(下)</h1><div class=post-meta><time datetime=2019-06-02T22:37:36+00:00 class="post-meta-item published dt-published"><svg viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M148 288h-40c-6.6.0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6.0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5.0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6.0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6.0 12 5.4 12 12v52h48c26.5.0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3.0 6-2.7 6-6z"/></svg>&nbsp;2019/6/2</time></div><div class="post-body e-content"><p style=text-indent:0><span class=drop-cap>上</span>篇了解了如何用TensorSlow构建计算图并完成前向传播，这里将继续了解计算损失和利用反向传播更新模型参数。</p><h2 id=tensorslow简介><a href=#tensorslow简介 class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>TensorSlow简介</h2><p>TensorSlow是极简的模仿TensorFlow的API的python包。</p><blockquote><p>本文是对原作者danielsabinasz的<a href=http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/ target=_blank rel=noopener>教程</a>的基础上，添加了一点自己的理解。</p><p>TensorSlow <a href=https://github.com/danielsabinasz/TensorSlow target=_blank rel=noopener><strong>Github repo地址</strong></a>
TensorSlow原作者<a href=http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/ target=_blank rel=noopener><strong>英文教程</strong></a></p><p><em>The source code has been built with maximal understandability in mind, rather than maximal efficiency.</em></p></blockquote><h2 id=perceptron-example><a href=#perceptron-example class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Perceptron Example</h2><p>本文中将会使用TensorSlow处理稍微复杂一点的模型。首先利用上篇的代码，搭建一个表征Perceptron的计算图。后面将以这个Perceptron为例，介绍TensorSlow如何进行损失计算和反向传播的。值得一提的是，这里的输入和参数已经是向量。</p><p><img src=https://pic.downk.cc/item/5e4d321648b86553eea745db.png alt=Perceptron计算图></p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Graph</span><span class=p>()</span><span class=o>.</span><span class=n>as_default</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>placeholder</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a weight matrix for 2 output classes:</span>
</span></span><span class=line><span class=cl><span class=c1># One with a weight vector (1, 1) for blue and </span>
</span></span><span class=line><span class=cl><span class=c1># one with a weight vector (-1, -1) for red</span>
</span></span><span class=line><span class=cl><span class=n>W</span> <span class=o>=</span> <span class=n>Variable</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>Variable</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>p</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span> <span class=n>add</span><span class=p>(</span><span class=n>matmul</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>W</span><span class=p>),</span> <span class=n>b</span><span class=p>)</span> <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>session</span> <span class=o>=</span> <span class=n>Session</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>output_probabilities</span> <span class=o>=</span> <span class=n>session</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>((</span><span class=n>blue_points</span><span class=p>,</span> <span class=n>red_points</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=p>})</span>
</span></span></code></pre></td></tr></table></div></div></div><h2 id=反向传播><a href=#反向传播 class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>反向传播</h2><p>上面构建的计算图，能够实现了Perceptron模型的前向传播。目前为止，模型的参数都是初值，已经给定的。这些仅仅为初值的参数效果好吗？那么需要用<strong>损失函数</strong>来评价当前参数。</p><h3 id=损失函数><a href=#损失函数 class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>损失函数</h3><p>对于二分类问题而言，最大似然估计(MLE)通常是比较好的参数估计的选择。MLE的损失函数形式是负对数似然，也就是交叉熵:</p><p>$$J = - \sum_{i=1}^N \sum_{j=1}^C c_{i, j} \cdot log p_{i, j}$$</p><p>其中，$p_{ij}$代表模型对第$i$个样本预测是第$j$个类别的分数，$c_{ij}$代表第$i$个样本是否是第$j$个类别。同样，损失函数$J$也对应于计算图中的一个operation节点：</p><p><img src=https://pic.downk.cc/item/5e4d321648b86553eea745d7.png alt=添加损失op></p><p>怎么建立这个operation节点？在原计算图的基础上，可以将节点$J$写成这样：
$$\sum_{i=1}^N \sum_{j=1}^C (c \odot log , p)_{i, j}$$</p><p>其中，$c=[c_{i1},\dots,c_{iC}]$是第$i$个样本的label的one-hot向量。可以发现，这是由多个基本的operation节点组成的。实现下面这些基础的operation节点并加以组合，即可得到节点$J$。</p><div class=table-container><table><thead><tr><th style=text-align:left>节点</th><th style=text-align:left>描述</th></tr></thead><tbody><tr><td style=text-align:left>$log$ 节点</td><td style=text-align:left>The element-wise logarithm<br>of a matrix or vector</td></tr><tr><td style=text-align:left>$\odot$ 节点</td><td style=text-align:left>The element-wise product<br>of two matrices</td></tr><tr><td style=text-align:left>$\sum_{j=1}^C$ 节点</td><td style=text-align:left>Sum over the<br>columns of a matrix</td></tr><tr><td style=text-align:left>$\sum_{i=1}^N$ 节点</td><td style=text-align:left>Sum over the<br>rows of a matrix</td></tr><tr><td style=text-align:left>$-$ 节点</td><td style=text-align:left>Taking the negative</td></tr></tbody></table></div><p>基础的operation节点写法在上篇已有介绍，这里仅以log节点为例，代码如下：</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>log</span><span class=p>(</span><span class=n>Operation</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Computes the natural logarithm of x element-wise.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>([</span><span class=n>x</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>compute</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x_value</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>x_value</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div><p>最后，组合这些operation节点，可以完整的给出节点$J$并计算出loss：</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Create a new graph</span>
</span></span><span class=line><span class=cl><span class=n>Graph</span><span class=p>()</span><span class=o>.</span><span class=n>as_default</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>placeholder</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>c</span> <span class=o>=</span> <span class=n>placeholder</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>W</span> <span class=o>=</span> <span class=n>Variable</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>Variable</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>p</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span> <span class=n>add</span><span class=p>(</span><span class=n>matmul</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>W</span><span class=p>),</span> <span class=n>b</span><span class=p>)</span> <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cross-entropy loss</span>
</span></span><span class=line><span class=cl><span class=n>J</span> <span class=o>=</span> <span class=n>negative</span><span class=p>(</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>multiply</span><span class=p>(</span><span class=n>c</span><span class=p>,</span> <span class=n>log</span><span class=p>(</span><span class=n>p</span><span class=p>)),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>session</span> <span class=o>=</span> <span class=n>Session</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>session</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>J</span><span class=p>,</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>((</span><span class=n>blue_points</span><span class=p>,</span> <span class=n>red_points</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>    <span class=n>c</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=p>[[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>]]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>blue_points</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=o>+</span> <span class=p>[[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>]]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>red_points</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=p>}))</span>
</span></span></code></pre></td></tr></table></div></div></div><h3 id=梯度下降><a href=#梯度下降 class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>梯度下降</h3><p>上面的步骤已经计算出模型损失函数。损失函数越小，意味着该模型参数下，模型输出和真实标签越接近。而搜索模型参数，使损失函数最小的方法叫做<strong>梯度下降</strong>，流程如下</p><blockquote><ol><li>参数$W$和$b$设置随机初始值。</li><li>计算$J$对$W$和$b$的梯度。</li><li>分别在其负梯度的方向上下降一小步(由<code>learning_rate</code>控制步长)。</li><li>回到step 2，继续执行，直到收敛。</li></ol></blockquote><p>而<code>GradientDescentOptimizer</code>就实现了上述流程中的step 3：</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>GradientDescentOptimizer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>learning_rate</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>learning_rate</span> <span class=o>=</span> <span class=n>learning_rate</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>minimize</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>loss</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>learning_rate</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>learning_rate</span>
</span></span><span class=line><span class=cl>        <span class=k>class</span> <span class=nc>MinimizationOperation</span><span class=p>(</span><span class=n>Operation</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>def</span> <span class=nf>compute</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>grad_table</span> <span class=o>=</span> <span class=n>compute_gradients</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                
</span></span><span class=line><span class=cl>                <span class=c1># Iterate all variables</span>
</span></span><span class=line><span class=cl>                <span class=k>for</span> <span class=n>node</span> <span class=ow>in</span> <span class=n>grad_table</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=k>if</span> <span class=nb>type</span><span class=p>(</span><span class=n>node</span><span class=p>)</span> <span class=o>==</span> <span class=n>Variable</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                        <span class=n>grad</span> <span class=o>=</span> <span class=n>grad_table</span><span class=p>[</span><span class=n>node</span><span class=p>]</span>            
</span></span><span class=line><span class=cl>                        <span class=c1># Take a step along the direction of the negative gradient</span>
</span></span><span class=line><span class=cl>                        <span class=n>node</span><span class=o>.</span><span class=n>value</span> <span class=o>-=</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>grad</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>MinimizationOperation</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div></div><p>上述代码中<code>compute_gradients</code>函数对应于step 2，将在下一个小节介绍。<code>grad_table</code>这个字典存放了损失函数$J$节点对图中所有Variable节点的当前梯度（因为只有Variable节点才需要更新）。</p><h3 id=梯度计算><a href=#梯度计算 class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>梯度计算</h3><p>梯度的计算根据导数的<strong>链式法则</strong>。</p><h4 id=链式法则><a href=#链式法则 class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>链式法则</h4><p>链式法则是微积分中的求导法则，用于求一个复合函数的导数。示例如下：</p><p><img src=https://pic.downk.cc/item/5e4d311f48b86553eea6f991.png alt=链式法则>
$$
\begin{aligned}
\frac{\partial e}{\partial a}
&= \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial a}\\
&= \frac{\partial e}{\partial d} \cdot \left( \frac{\partial d}{\partial b} \cdot \frac{\partial b}{\partial a} + \frac{\partial d}{\partial c} \cdot \frac{\partial c}{\partial a} \right)\\
&= \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial b} \cdot \frac{\partial b}{\partial a} + \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial c} \cdot \frac{\partial c}{\partial a}
\end{aligned}
$$</p><h3 id=梯度计算-1><a href=#梯度计算-1 class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>梯度计算</h3><p>通过上面的链式法则，计算损失函数节点（记为loss节点）对当前节点$n$的梯度，也是链式的：</p><ol><li>计算loss节点对当前$n$节点的consumer节点的输出的梯度$G$.</li><li>计算$n$节点的consumer节点对$n$节点梯度$\times G$，将所有consumer节点计算出的梯度全部相加。</li></ol><pre tabindex=0><code># 计算当前节点梯度的伪代码
grad_n = grad_child1 * grad_child1_n + ... 
        + grad_childk * grad_childk_n
</code></pre><ul><li><code>grad_n</code> : $\partial\text{loss}/\partial{n}$</li><li><code>chlid1</code>: a child node of node $n$</li><li><code>grad_child1</code>: $\partial\text{loss}/\partial{\text{child1}}$</li><li><code>grad_child1_n</code>: $\partial\text{child1}/\partial{n}$</li></ul><p>先计算子节点梯度，然后得到当前节点梯度，完全按照链式法则。由于当前节点的梯度计算依赖子节点的梯度计算，因此可以使用拓扑排序。而作者使用了BFS来完成拓扑排序。从反图角度而言，每个节点的入度都为0，可以放心使用BFS来完成拓扑排序。</p><p>通过一步步迭代，能够得到所有节点的梯度。其中$\partial\text{child1}/\partial{n}$，也就是子节点的输出对于该节点的输出的梯度如何计算？这个工作应该由子节点来完成。</p><p>对于一个operation节点，提前定义这个operation的梯度计算函数，并使用装饰器<code>@RegisterGradient</code>来将实现了计算梯度函数的opeartion节点注册到全局变量<code>_gradient_registry</code>中，具体细节见下一小节。整体的梯度计算过程如下：</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>queue</span> <span class=kn>import</span> <span class=n>Queue</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_gradients</span><span class=p>(</span><span class=n>loss</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>grad_table</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>    <span class=n>grad_table</span><span class=p>[</span><span class=n>loss</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Perform a breadth-first search, backwards from the loss</span>
</span></span><span class=line><span class=cl>    <span class=n>visited</span> <span class=o>=</span> <span class=nb>set</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>queue</span> <span class=o>=</span> <span class=n>Queue</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>visited</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>queue</span><span class=o>.</span><span class=n>put</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=ow>not</span> <span class=n>queue</span><span class=o>.</span><span class=n>empty</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>node</span> <span class=o>=</span> <span class=n>queue</span><span class=o>.</span><span class=n>get</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>node</span> <span class=o>!=</span> <span class=n>loss</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>grad_table</span><span class=p>[</span><span class=n>node</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>            <span class=c1># Iterate all consumers</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>consumer</span> <span class=ow>in</span> <span class=n>node</span><span class=o>.</span><span class=n>consumers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>lossgrad_wrt_consumer_output</span> <span class=o>=</span> <span class=n>grad_table</span><span class=p>[</span><span class=n>consumer</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=c1># Retrieve the function which computes gradients with respect to</span>
</span></span><span class=line><span class=cl>                <span class=c1># consumer&#39;s inputs given gradients with respect to consumer&#39;s output.</span>
</span></span><span class=line><span class=cl>                <span class=n>consumer_op_type</span> <span class=o>=</span> <span class=n>consumer</span><span class=o>.</span><span class=vm>__class__</span>
</span></span><span class=line><span class=cl>                <span class=n>bprop</span> <span class=o>=</span> <span class=n>_gradient_registry</span><span class=p>[</span><span class=n>consumer_op_type</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=c1># Get the gradient of the loss with respect to all of consumer&#39;s inputs</span>
</span></span><span class=line><span class=cl>                <span class=n>lossgrads_wrt_consumer_inputs</span> <span class=o>=</span> <span class=n>bprop</span><span class=p>(</span><span class=n>consumer</span><span class=p>,</span> <span class=n>lossgrad_wrt_consumer_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>consumer</span><span class=o>.</span><span class=n>input_nodes</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=c1># If there is a single input node to the consumer, lossgrads_wrt_consumer_inputs is a scalar</span>
</span></span><span class=line><span class=cl>                    <span class=n>grad_table</span><span class=p>[</span><span class=n>node</span><span class=p>]</span> <span class=o>+=</span> <span class=n>lossgrads_wrt_consumer_inputs</span>
</span></span><span class=line><span class=cl>                <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=c1># Otherwise, lossgrads_wrt_consumer_inputs is an array of gradients for each input node</span>
</span></span><span class=line><span class=cl>                    <span class=n>node_index_in_consumer_inputs</span> <span class=o>=</span> <span class=n>consumer</span><span class=o>.</span><span class=n>input_nodes</span><span class=o>.</span><span class=n>index</span><span class=p>(</span><span class=n>node</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=c1># Get the gradient of the loss with respect to node</span>
</span></span><span class=line><span class=cl>                    <span class=n>lossgrad_wrt_node</span> <span class=o>=</span> <span class=n>lossgrads_wrt_consumer_inputs</span><span class=p>[</span><span class=n>node_index_in_consumer_inputs</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                    <span class=n>grad_table</span><span class=p>[</span><span class=n>node</span><span class=p>]</span> <span class=o>+=</span> <span class=n>lossgrad_wrt_node</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>hasattr</span><span class=p>(</span><span class=n>node</span><span class=p>,</span> <span class=s2>&#34;input_nodes&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>input_node</span> <span class=ow>in</span> <span class=n>node</span><span class=o>.</span><span class=n>input_nodes</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=ow>not</span> <span class=n>input_node</span> <span class=ow>in</span> <span class=n>visited</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>visited</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>input_node</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>queue</span><span class=o>.</span><span class=n>put</span><span class=p>(</span><span class=n>input_node</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>grad_table</span>
</span></span></code></pre></td></tr></table></div></div></div><p>其中一些细节还需要自己体会代码。</p><h3 id=operation节点的梯度><a href=#operation节点的梯度 class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Operation节点的梯度</h3><p>根据operation操作的<code>compute</code>方法给出节点正向传播的函数，据此计算梯度函数，然后注册到全局变量中即可。</p><p>以矩阵乘法<code>matmul</code>为例：给定对于$AB$的梯度$G$，其对于$A$的梯度是$GB^T$，对于$B$的梯度是$A^TG$，所以该节点的梯度是一个向量。</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@RegisterGradient</span><span class=p>(</span><span class=s2>&#34;matmul&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>_matmul_gradient</span><span class=p>(</span><span class=n>op</span><span class=p>,</span> <span class=n>grad</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>A</span> <span class=o>=</span> <span class=n>op</span><span class=o>.</span><span class=n>inputs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>B</span> <span class=o>=</span> <span class=n>op</span><span class=o>.</span><span class=n>inputs</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span><span class=n>grad</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>B</span><span class=o>.</span><span class=n>T</span><span class=p>),</span> <span class=n>A</span><span class=o>.</span><span class=n>T</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>grad</span><span class=p>)]</span>
</span></span></code></pre></td></tr></table></div></div></div><p>同样的<code>sigmoid</code>的梯度可以写为$G \cdot \sigma(a) \cdot \sigma(1-a)$</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@RegisterGradient</span><span class=p>(</span><span class=s2>&#34;sigmoid&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>_sigmoid_gradient</span><span class=p>(</span><span class=n>op</span><span class=p>,</span> <span class=n>grad</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>sigmoid</span> <span class=o>=</span> <span class=n>op</span><span class=o>.</span><span class=n>output</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>grad</span> <span class=o>*</span> <span class=n>sigmoid</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>sigmoid</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div><h2 id=perceptron-train><a href=#perceptron-train class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Perceptron Train</h2><p>完整的Perceptron模型训练代码如下：</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Create a new graph</span>
</span></span><span class=line><span class=cl><span class=n>Graph</span><span class=p>()</span><span class=o>.</span><span class=n>as_default</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>placeholder</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>c</span> <span class=o>=</span> <span class=n>placeholder</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize weights randomly: step 1</span>
</span></span><span class=line><span class=cl><span class=n>W</span> <span class=o>=</span> <span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>p</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span> <span class=n>add</span><span class=p>(</span><span class=n>matmul</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>W</span><span class=p>),</span> <span class=n>b</span><span class=p>)</span> <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>J</span> <span class=o>=</span> <span class=n>negative</span><span class=p>(</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>multiply</span><span class=p>(</span><span class=n>c</span><span class=p>,</span> <span class=n>log</span><span class=p>(</span><span class=n>p</span><span class=p>)),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># step 2 and step 3</span>
</span></span><span class=line><span class=cl><span class=n>minimization_op</span> <span class=o>=</span> <span class=n>GradientDescentOptimizer</span><span class=p>(</span><span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.01</span><span class=p>)</span><span class=o>.</span><span class=n>minimize</span><span class=p>(</span><span class=n>J</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>feed_dict</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>((</span><span class=n>blue_points</span><span class=p>,</span> <span class=n>red_points</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>    <span class=n>c</span><span class=p>:</span>  <span class=p>[[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>]]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>blue_points</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=o>+</span> <span class=p>[[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>]]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>red_points</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>session</span> <span class=o>=</span> <span class=n>Session</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># Perform 100 gradient descent steps, step 4</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>J_value</span> <span class=o>=</span> <span class=n>session</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>J</span><span class=p>,</span> <span class=n>feed_dict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>step</span> <span class=o>%</span> <span class=mi>10</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Step:&#34;</span><span class=p>,</span> <span class=n>step</span><span class=p>,</span> <span class=s2>&#34; Loss:&#34;</span><span class=p>,</span> <span class=n>J_value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>session</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>minimization_op</span><span class=p>,</span> <span class=n>feed_dict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>W_value</span> <span class=o>=</span> <span class=n>session</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>W</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Weight matrix:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>W_value</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b_value</span> <span class=o>=</span> <span class=n>session</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Bias:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>b_value</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div><pre><code>Step: 0  Loss: 202.782788396
Step: 10  Loss: 4.04566479054
Step: 20  Loss: 2.69644468305
Step: 30  Loss: 2.00506261735
Step: 40  Loss: 1.62202006027
Step: 50  Loss: 1.39268559111
Step: 60  Loss: 1.24498439759
Step: 70  Loss: 1.14348265257
Step: 80  Loss: 1.06965484385
Step: 90  Loss: 1.01324253829
Weight matrix:
 [[ 1.27496197 -1.77251219]
 [ 1.11820232 -2.01586474]]
Bias:
 [-0.45274057 -0.39071841]
</code></pre><p>可以发现loss的确是不断降低的，模型参数不断优化更新。</p><h2 id=multi-layer-perceptrons><a href=#multi-layer-perceptrons class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Multi-Layer Perceptrons</h2><p>使用TensorSlow来处理更加复杂的问题：分类的决策边界更加复杂。这里也搭建了更加复杂的模型MLP。</p><h3 id=数据分布><a href=#数据分布 class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>数据分布</h3><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorslow</span> <span class=k>as</span> <span class=nn>ts</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create two clusters of red points centered at (0, 0) and (1, 1), respectively.</span>
</span></span><span class=line><span class=cl><span class=n>red_points</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>((</span>
</span></span><span class=line><span class=cl>    <span class=mf>0.2</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>25</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]]</span> <span class=o>*</span> <span class=mi>25</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=mf>0.2</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>25</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]]</span> <span class=o>*</span> <span class=mi>25</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create two clusters of blue points centered at (0, 1) and (1, 0), respectively.</span>
</span></span><span class=line><span class=cl><span class=n>blue_points</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>((</span>
</span></span><span class=line><span class=cl>    <span class=mf>0.2</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>25</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>]]</span> <span class=o>*</span> <span class=mi>25</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=mf>0.2</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>25</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>]]</span> <span class=o>*</span> <span class=mi>25</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot them</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>red_points</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>red_points</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>blue_points</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>blue_points</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;blue&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div></div><p><img src=https://pic.downk.cc/item/5e4d393a48b86553eea9d311.png alt=真实数据分布></p><h3 id=计算图><a href=#计算图 class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>计算图</h3><p><img src=https://pic.downk.cc/item/5e4d393a48b86553eea9d313.png alt=MLP计算图></p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Create a new graph</span>
</span></span><span class=line><span class=cl><span class=n>ts</span><span class=o>.</span><span class=n>Graph</span><span class=p>()</span><span class=o>.</span><span class=n>as_default</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create training input placeholder</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>placeholder</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># Create placeholder for the training classes</span>
</span></span><span class=line><span class=cl><span class=n>c</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>placeholder</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build a hidden layer</span>
</span></span><span class=line><span class=cl><span class=n>W_hidden1</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>b_hidden1</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>p_hidden1</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>ts</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>ts</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>W_hidden1</span><span class=p>),</span> <span class=n>b_hidden1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build a hidden layer</span>
</span></span><span class=line><span class=cl><span class=n>W_hidden2</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>b_hidden2</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>p_hidden2</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>ts</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>ts</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>p_hidden1</span><span class=p>,</span> <span class=n>W_hidden2</span><span class=p>),</span> <span class=n>b_hidden2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build a hidden layer</span>
</span></span><span class=line><span class=cl><span class=n>W_hidden3</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>b_hidden3</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>p_hidden3</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>ts</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>ts</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>p_hidden2</span><span class=p>,</span> <span class=n>W_hidden3</span><span class=p>),</span> <span class=n>b_hidden3</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build the output layer</span>
</span></span><span class=line><span class=cl><span class=n>W_output</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>b_output</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>p_output</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>ts</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>ts</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>p_hidden3</span><span class=p>,</span> <span class=n>W_output</span><span class=p>),</span> <span class=n>b_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build cross-entropy loss</span>
</span></span><span class=line><span class=cl><span class=n>J</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>negative</span><span class=p>(</span><span class=n>ts</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>ts</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>ts</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=n>c</span><span class=p>,</span> <span class=n>ts</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>p_output</span><span class=p>)),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build minimization op</span>
</span></span><span class=line><span class=cl><span class=n>minimization_op</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>GradientDescentOptimizer</span><span class=p>(</span><span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.03</span><span class=p>)</span><span class=o>.</span><span class=n>minimize</span><span class=p>(</span><span class=n>J</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build placeholder inputs</span>
</span></span><span class=line><span class=cl><span class=n>feed_dict</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>((</span><span class=n>blue_points</span><span class=p>,</span> <span class=n>red_points</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>    <span class=n>c</span><span class=p>:</span>  <span class=p>[[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>]]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>blue_points</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=o>+</span> <span class=p>[[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>]]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>red_points</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create session</span>
</span></span><span class=line><span class=cl><span class=n>session</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>Session</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Perform 100 gradient descent steps</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>J_value</span> <span class=o>=</span> <span class=n>session</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>J</span><span class=p>,</span> <span class=n>feed_dict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>step</span> <span class=o>%</span> <span class=mi>100</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Step:&#34;</span><span class=p>,</span> <span class=n>step</span><span class=p>,</span> <span class=s2>&#34; Loss:&#34;</span><span class=p>,</span> <span class=n>J_value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>session</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>minimization_op</span><span class=p>,</span> <span class=n>feed_dict</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div><pre><code>Step: 0  Loss: 105.25316761015766
Step: 100  Loss: 54.82276887616324
Step: 200  Loss: 18.531741905961816
Step: 300  Loss: 10.88319073941583
Step: 400  Loss: 5.167908735173651
Step: 500  Loss: 4.015258056948531
...
Step: 1400  Loss: 0.1973992659737359
Step: 1500  Loss: 0.17700496511514013
Step: 1600  Loss: 0.1602628699213534
Step: 1700  Loss: 0.14629349101006833
Step: 1800  Loss: 0.13447502395360536
Step: 1900  Loss: 0.1243562427509077
</code></pre><h3 id=可视化><a href=#可视化 class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>可视化</h3><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Visualize classification boundary</span>
</span></span><span class=line><span class=cl><span class=n>xs</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ys</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pred_classes</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>xs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>y</span> <span class=ow>in</span> <span class=n>ys</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>pred_class</span> <span class=o>=</span> <span class=n>session</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>p_output</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                 <span class=n>feed_dict</span><span class=o>=</span><span class=p>{</span><span class=n>X</span><span class=p>:</span> <span class=p>[[</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>]]})[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>pred_classes</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>pred_class</span><span class=o>.</span><span class=n>argmax</span><span class=p>()))</span>
</span></span><span class=line><span class=cl><span class=n>xs_p</span><span class=p>,</span> <span class=n>ys_p</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>xs_n</span><span class=p>,</span> <span class=n>ys_n</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>pred_classes</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>c</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>xs_n</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>ys_n</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>xs_p</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>ys_p</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>xs_p</span><span class=p>,</span> <span class=n>ys_p</span><span class=p>,</span> <span class=s1>&#39;ro&#39;</span><span class=p>,</span> <span class=n>xs_n</span><span class=p>,</span> <span class=n>ys_n</span><span class=p>,</span> <span class=s1>&#39;bo&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div></div><p><img src=https://pic.downk.cc/item/5e4d393a48b86553eea9d30f.png alt=决策边界></p><h2 id=小结><a href=#小结 class=anchor-link><svg viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>小结</h2><p>短小精悍的TensorSlow揭示了了深度模型框架的基本工作机理。当然，它只适用于教学，生产环境下的深度学习框架软件将更加复杂。</p></div></article><div class=post-gitinfo><div class=post-gitinfo-left><div class="gitinfo-item commit"><a href=https://github.com/yinyajun/yinyajun.github.io/commit/7d3fabff8ecab4f6583abc079727fab0c95e0d1f target=_blank rel=noopener><svg viewBox="0 0 384 512" class="icon git-icon"><path d="M384 144c0-44.2-35.8-80-80-80s-80 35.8-80 80c0 36.4 24.3 67.1 57.5 76.8-.6 16.1-4.2 28.5-11 36.9-15.4 19.2-49.3 22.4-85.2 25.7-28.2 2.6-57.4 5.4-81.3 16.9v-144c32.5-10.2 56-40.5 56-76.3.0-44.2-35.8-80-80-80S0 35.8.0 80c0 35.8 23.5 66.1 56 76.3v199.3C23.5 365.9.0 396.2.0 432c0 44.2 35.8 80 80 80s80-35.8 80-80c0-34-21.2-63.1-51.2-74.6 3.1-5.2 7.8-9.8 14.9-13.4 16.2-8.2 40.4-10.4 66.1-12.8 42.2-3.9 90-8.4 118.2-43.4 14-17.4 21.1-39.8 21.6-67.9 31.6-10.8 54.4-40.7 54.4-75.9zM80 64c8.8.0 16 7.2 16 16s-7.2 16-16 16-16-7.2-16-16 7.2-16 16-16zm0 384c-8.8.0-16-7.2-16-16s7.2-16 16-16 16 7.2 16 16-7.2 16-16 16zm224-320c8.8.0 16 7.2 16 16s-7.2 16-16 16-16-7.2-16-16 7.2-16 16-16z"/></svg>7d3fabf</a></div></div><div class=post-gitinfo-right><div class="gitinfo-item feedback"><a href=https://github.com/yinyajun/yinyajun.github.io/issues target=_blank rel=noopener><svg viewBox="0 0 384 512" class="icon feedback-icon"><path d="M202.021.0C122.202.0 70.503 32.703 29.914 91.026c-7.363 10.58-5.093 25.086 5.178 32.874l43.138 32.709c10.373 7.865 25.132 6.026 33.253-4.148 25.049-31.381 43.63-49.449 82.757-49.449 30.764.0 68.816 19.799 68.816 49.631.0 22.552-18.617 34.134-48.993 51.164-35.423 19.86-82.299 44.576-82.299 106.405V320c0 13.255 10.745 24 24 24h72.471c13.255.0 24-10.745 24-24v-5.773c0-42.86 125.268-44.645 125.268-160.627C377.504 66.256 286.902.0 202.021.0zM192 373.459c-38.196.0-69.271 31.075-69.271 69.271.0 38.195 31.075 69.27 69.271 69.27s69.271-31.075 69.271-69.271-31.075-69.27-69.271-69.27z"/></svg>反馈</a></div><div class="gitinfo-item edit"><a href=https://github.com/yinyajun/yinyajun.github.io/tree/main/content/posts/ai/basic/tensorslow02.md target=_blank rel=noopener><svg viewBox="0 0 576 512" class="icon edit-icon"><path d="M402.3 344.9l32-32c5-5 13.7-1.5 13.7 5.7V464c0 26.5-21.5 48-48 48H48c-26.5.0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h273.5c7.1.0 10.7 8.6 5.7 13.7l-32 32c-1.5 1.5-3.5 2.3-5.7 2.3H48v352h352V350.5c0-2.1.8-4.1 2.3-5.6zm156.6-201.8L296.3 405.7l-90.4 10c-26.2 2.9-48.5-19.2-45.6-45.6l10-90.4L432.9 17.1c22.9-22.9 59.9-22.9 82.7.0l43.2 43.2c22.9 22.9 22.9 60 .1 82.8zM460.1 174 402 115.9 216.2 301.8l-7.3 65.3 65.3-7.3L460.1 174zm64.8-79.7-43.2-43.2c-4.1-4.1-10.8-4.1-14.8.0L436 82l58.1 58.1 30.9-30.9c4-4.2 4-10.8-.1-14.9z"/></svg>编辑</a></div></div></div><div class=related-posts><h2 class=related-title>相关文章：</h2><ul class=related-list><li class=related-item><a href=/posts/ai/basic/tensorslow01/ class=related-link>有趣的TensorSlow(上)</a></li></ul></div><footer class=minimal-footer><div class=post-tag><a href=/tags/tensorslow/ rel=tag class=post-tag-link>#tensorslow</a></div><div class=post-category><a href=/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/ class=post-category-link>人工智能</a></div></footer></div></main><div id=back-to-top class=back-to-top><a href=#><svg viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6.0-33.9L207 39c9.4-9.4 24.6-9.4 33.9.0l194.3 194.3c9.4 9.4 9.4 24.6.0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3.0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a></div></div><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js")})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css><script>if(typeof renderMathInElement=="undefined"){const e=e=>{const t=document.createElement("script");t.defer=!0,t.crossOrigin="anonymous",Object.keys(e).forEach(n=>{t[n]=e[n]}),document.body.appendChild(t)};e({src:"https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js",onload:()=>{e({src:"https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/mhchem.min.js",onload:()=>{e({src:"https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js",onload:()=>{renderKaTex()}})}})}})}else renderKaTex();function renderKaTex(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})}</script></body></html>